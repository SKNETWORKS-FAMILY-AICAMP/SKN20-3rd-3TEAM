"""
ë””ë²„ê¹… ë¡œê·¸ë¥¼ í¬í•¨í•œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
"""
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

# í™˜ê²½ì„¤ì •
import os
from dotenv import load_dotenv
load_dotenv()

# ëª¨ë“ˆ import
from src.embeddings import get_embedding_model, load_vectorstore
from src.retrieval import create_retriever
from src.pipeline import LangGraphRAGPipeline

print("="*80)
print("LangGraph CRAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
print("="*80)

try:
    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("\n[1/3] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = get_embedding_model("openai")
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # 2. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    print("[2/3] ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
    vectorstore = load_vectorstore(
        embedding_model,
        persist_directory="./chroma_db",
        collection_name="rag_collection"
    )
    print("âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ\n")
    
    # 3. Retriever ìƒì„±
    print("[3/3] Retriever ìƒì„± ì¤‘...")
    retriever = create_retriever(
        vectorstore,
        k=10,
        rerank_k=5,
        use_reranking=True,
        embedding_model=embedding_model
    )
    print("âœ… Retriever ìƒì„± ì™„ë£Œ\n")
    
    # LangGraph CRAG íŒŒì´í”„ë¼ì¸ ìƒì„±
    print("\n" + "="*80)
    print("LangGraph CRAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
    print("="*80)
    pipeline = LangGraphRAGPipeline(retriever, debug=True)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("\n\n" + "#"*80)
    print("# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: ê°•ì•„ì§€ ë‘ë“œëŸ¬ê¸° ì¦ìƒ")
    print("#"*80)
    
    result = pipeline.rag_pipeline_with_sources("ê°•ì•„ì§€ ëª¸ì— ë‘ë“œëŸ¬ê¸°ê°€ ë‚¬ì–´ìš”. ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œìš”?")
    
    print("\n\n" + "="*80)
    print("ğŸ“‹ ìµœì¢… ë‹µë³€")
    print("="*80)
    print(result['answer'])
    print("\n")

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    import traceback
    traceback.print_exc()

