"""
ğŸ¾ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ RAG ì–´ì‹œìŠ¤í„´íŠ¸ - LangGraph + CRAG íŒ¨í„´
LangGraph ê¸°ë°˜ì˜ CRAG(Corrective RAG) íŒ¨í„´ êµ¬í˜„
ì›¹ ê²€ìƒ‰ì€ Tavily API ì‚¬ìš©
"""

import os
import json
import warnings
warnings.filterwarnings("ignore")

from typing import List, Literal
from typing_extensions import TypedDict
from dotenv import load_dotenv
from pathlib import Path

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.retrievers import TavilySearchAPIRetriever

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, START, END

# Pydantic
from pydantic import BaseModel, Field

# í™˜ê²½ì„¤ì •
load_dotenv()

if not os.environ.get('OPENAI_API_KEY'):
    raise ValueError('âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜')

if not os.environ.get('TAVILY_API_KEY'):
    raise ValueError('âŒ TAVILY_API_KEY í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í•„ìˆ˜')

print("âœ… API í‚¤ í™•ì¸ ì™„ë£Œ\n")

# ============================================================================
# 1ï¸âƒ£ ìƒíƒœ ì •ì˜ (State)
# ============================================================================

class PetMedicalState(TypedDict):
    """ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ RAG ìƒíƒœ"""
    question: str                          # ì‚¬ìš©ì ì§ˆë¬¸
    documents: List[Document]              # ë²¡í„° ì €ì¥ì†Œì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ
    filtered_documents: List[Document]     # ê´€ë ¨ì„± í‰ê°€ë¥¼ í†µê³¼í•œ ë¬¸ì„œ
    web_search_needed: str                 # ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ (Yes/No)
    context: str                           # ë‹µë³€ ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸
    answer: str                            # ìµœì¢… ë‹µë³€
    grade_results: List[str]               # ê° ë¬¸ì„œì˜ í‰ê°€ ê²°ê³¼
    classification: str                    # ì§ˆë¬¸ ë¶„ë¥˜ (ì˜ë£Œ/ë³‘ì›/ì¼ë°˜)
    sources: List[dict]                    # ë‹µë³€ì— ì‚¬ìš©ëœ ì¶œì²˜

# ============================================================================
# 2ï¸âƒ£ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ëª¨ë¸
# ============================================================================

class GradeDocuments(BaseModel):
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼"""
    binary_score: str = Field(
        description="ë¬¸ì„œê°€ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'yes', ì—†ìœ¼ë©´ 'no'"
    )

# ============================================================================
# 3ï¸âƒ£ ì§ˆë³‘ ë°ì´í„° ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
# ============================================================================

print("ğŸ“š ë°˜ë ¤ë™ë¬¼ ì§ˆë³‘ ë°ì´í„° ë¡œë“œ ì¤‘...")

# JSON íŒŒì¼ ê²½ë¡œ
disease_dir = Path("data/raw/disease")

# ì§ˆë³‘ ë°ì´í„° ë¡œë“œ
documents = []
if disease_dir.exists():
    for json_file in disease_dir.glob("*.json"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                disease_data = json.load(f)
                
                # ë¬¸ì„œ ë‚´ìš© êµ¬ì„±
                doc_content = f"""
ì§ˆë³‘ëª…: {disease_data.get('name', 'N/A')}
ì„¤ëª…: {disease_data.get('description', 'N/A')}

ì¦ìƒ:
{', '.join(disease_data.get('symptoms', []))}

ì›ì¸:
{', '.join(disease_data.get('causes', []))}

ì¹˜ë£Œ:
{', '.join(disease_data.get('treatment', []))}

ì˜ˆë°©:
{', '.join(disease_data.get('prevention', []))}

ì–¸ì œ ë³‘ì›ì„ ë°©ë¬¸í•´ì•¼ í•˜ë‚˜ìš”:
{', '.join(disease_data.get('when_to_visit_vet', []))}

ê°€ì • ê´€ë¦¬:
{', '.join(disease_data.get('home_care', []))}
"""
                
                documents.append(
                    Document(
                        page_content=doc_content,
                        metadata={
                            "source": "internal_database",
                            "disease_name": disease_data.get('name', ''),
                            "file": json_file.name
                        }
                    )
                )
        except Exception as e:
            print(f"   âš ï¸  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({json_file.name}): {e}")

print(f"âœ… {len(documents)}ê°œ ì§ˆë³‘ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n")

# í…ìŠ¤íŠ¸ ë¶„í• 
print("ğŸ”ª ë¬¸ì„œ ë¶„í•  ì¤‘...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)
doc_splits = text_splitter.split_documents(documents)
print(f"âœ… {len(doc_splits)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ\n")

# ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
print("ğŸ—‚ï¸  Chroma ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...")
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name='pet_medical_crag',
    embedding=OpenAIEmbeddings(model='text-embedding-3-small'),
    persist_directory="./chroma_pet_medical"
)
print(f"âœ… ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ\n")

# ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

# ============================================================================
# 4ï¸âƒ£ LLM ë° Grader ì„¤ì •
# ============================================================================

print("ğŸ¤– LLM ë° Grader ì´ˆê¸°í™” ì¤‘...\n")

# ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ Grader
grader_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_grader = grader_llm.with_structured_output(GradeDocuments)

grade_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í‰ê°€ ê¸°ì¤€:
- ë¬¸ì„œê°€ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë‚˜ ì˜ë¯¸ì™€ ì—°ê´€ë˜ì–´ ìˆìœ¼ë©´ 'ê´€ë ¨ìˆìŒ'ìœ¼ë¡œ í‰ê°€
- ë‹µë³€ì— ë„ì›€ì´ ë  ê°€ëŠ¥ì„±ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆìœ¼ë©´ 'ê´€ë ¨ìˆìŒ'
- ì™„ì „íˆ ë¬´ê´€í•œ ë‚´ìš©ì´ë©´ 'ê´€ë ¨ì—†ìŒ'

ê´€ëŒ€í•˜ê²Œ í‰ê°€í•˜ê³ , ì•½ê°„ì˜ ì—°ê´€ì„±ì´ë¼ë„ ìˆìœ¼ë©´ 'yes'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”."""),
    ("human", """ì§ˆë¬¸: {question}
 
ë¬¸ì„œ ë‚´ìš©:
{document}

ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µí•˜ì„¸ìš”""")
])

document_grader = grade_prompt | structured_grader

# ë‹µë³€ ìƒì„± LLM
generation_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

# ì§ˆë¬¸ ë¶„ë¥˜ LLM
classification_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ============================================================================
# 5ï¸âƒ£ ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# ============================================================================

def classify_question_node(state: PetMedicalState) -> dict:
    """
    ì§ˆë¬¸ ë¶„ë¥˜ ë…¸ë“œ
    ì˜ë£Œ/ë³‘ì›/ì¼ë°˜ ì§ˆë¬¸ ë¶„ë¥˜
    """
    print("\n   [CLASSIFY ë…¸ë“œ] ì§ˆë¬¸ ë¶„ë¥˜ ì¤‘...")
    
    question = state['question']
    
    classify_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        
ì§ˆë¬¸ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- medical: ë°˜ë ¤ë™ë¬¼ì˜ ê±´ê°•, ì§ˆë³‘, ì¦ìƒ, ì¹˜ë£Œ ë“± ì˜ë£Œ ê´€ë ¨
- hospital: ë™ë¬¼ë³‘ì› ì°¾ê¸°, ìœ„ì¹˜, ì§„ë£Œì‹œê°„ ë“± ë³‘ì›/ì§€ë„ ê´€ë ¨
- general: ë°˜ë ¤ë™ë¬¼ ê¸°ë³¸ ê´€ë¦¬, í›ˆë ¨, ì—¬í–‰ ë“± ì¼ë°˜ ì •ë³´

ë¶„ë¥˜ë§Œ ë°˜í™˜í•˜ì„¸ìš” (medical/hospital/general ì¤‘ í•˜ë‚˜)"""),
        ("human", f"ì§ˆë¬¸: {question}\n\në¶„ë¥˜:")
    ])
    
    chain = classify_prompt | classification_llm | StrOutputParser()
    classification = chain.invoke({}).lower().strip()
    
    # ë¶„ë¥˜ ê²°ê³¼ ê²€ì¦
    if classification not in ['medical', 'hospital', 'general']:
        classification = 'general'
    
    print(f"   â†’ ë¶„ë¥˜: {classification}")
    
    return {
        "classification": classification,
        "question": question
    }

def retrieve_node(state: PetMedicalState) -> dict:
    """
    ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
    ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    """
    print("   [RETRIEVE ë…¸ë“œ] ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    
    question = state['question']
    
    # ì˜ë£Œ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ê²€ìƒ‰ ìŠ¤í‚µ
    if state.get('classification', 'general') != 'medical':
        print("   â†’ ì˜ë£Œ ì§ˆë¬¸ ì•„ë‹˜, ë¬¸ì„œ ê²€ìƒ‰ ìŠ¤í‚µ")
        return {
            'documents': [],
            'question': question
        }
    
    documents = retriever.invoke(question)
    print(f"   â†’ {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
    
    return {
        'documents': documents,
        'question': question
    }

def grade_documents_node(state: PetMedicalState) -> dict:
    """
    ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ë…¸ë“œ
    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ LLMìœ¼ë¡œ í‰ê°€
    """
    print("   [GRADE ë…¸ë“œ] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì¤‘...")
    
    question = state['question']
    documents = state['documents']
    classification = state.get('classification', 'general')
    
    # ì˜ë£Œ ì§ˆë¬¸ ì•„ë‹ˆê±°ë‚˜ ë¬¸ì„œ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰
    if classification != 'medical' or len(documents) == 0:
        web_search_needed = "Yes"
        print(f"   â†’ ì›¹ ê²€ìƒ‰ í•„ìš”! (ì˜ë£Œ: {classification == 'medical'}, ë¬¸ì„œ: {len(documents)}ê°œ)")
        return {
            "filtered_documents": [],
            "web_search_needed": web_search_needed,
            "grade_results": []
        }
    
    filtered_docs = []
    grade_results = []
    
    for i, doc in enumerate(documents, 1):
        try:
            score = document_grader.invoke({
                'question': question,
                'document': doc.page_content[:1000]  # ë¬¸ì„œ ì¼ë¶€ë§Œ í‰ê°€
            })
            grade = score.binary_score.lower()
            
            if 'yes' in grade:
                filtered_docs.append(doc)
                grade_results.append("relevant")
                print(f"   â†’ [{i}] âœ… ê´€ë ¨ìˆìŒ")
            else:
                grade_results.append("not_relevant")
                print(f"   â†’ [{i}] âŒ ê´€ë ¨ì—†ìŒ")
        except Exception as e:
            print(f"   â†’ [{i}] âš ï¸  í‰ê°€ ì˜¤ë¥˜: {e}")
            grade_results.append("error")
    
    # ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
    if len(filtered_docs) == 0:
        web_search_needed = "Yes"
        print(f"   â†’ ê´€ë ¨ ë¬¸ì„œ 0ê°œ â†’ ì›¹ ê²€ìƒ‰ í•„ìš”!")
    else:
        web_search_needed = "No"
        print(f"   â†’ {len(filtered_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ í™•ë³´!")
    
    return {
        "filtered_documents": filtered_docs,
        "web_search_needed": web_search_needed,
        "grade_results": grade_results
    }

def web_search_node(state: PetMedicalState) -> dict:
    """
    ì›¹ ê²€ìƒ‰ ë…¸ë“œ
    Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰
    """
    print("   [WEB_SEARCH ë…¸ë“œ] Tavilyë¡œ ì›¹ ê²€ìƒ‰ ì¤‘...")
    
    try:
        question = state["question"]
        
        # Tavily ê²€ìƒ‰ ìˆ˜í–‰
        web_search = TavilySearchAPIRetriever(k=3)
        web_results = web_search.invoke(question)
        
        print(f"   â†’ {len(web_results)}ê°œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ íšë“")
        
        # ê¸°ì¡´ í•„í„°ë§ëœ ë¬¸ì„œì— ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        filtered_docs = state.get("filtered_documents", [])
        for doc in web_results:
            doc.metadata["source"] = "web_search"
            doc.metadata["type"] = "external"
            filtered_docs.append(doc)
        
        print(f"   â†’ ì´ {len(filtered_docs)}ê°œ ë¬¸ì„œë¡œ í†µí•©")
        
        return {
            "filtered_documents": filtered_docs
        }
    
    except Exception as e:
        print(f"   âš ï¸  ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        print("   â†’ ê¸°ì¡´ ë¬¸ì„œë¡œ ë‹µë³€ ìƒì„± ì§„í–‰")
        return {
            "filtered_documents": state.get("filtered_documents", [])
        }

def generate_node(state: PetMedicalState) -> dict:
    """
    ë‹µë³€ ìƒì„± ë…¸ë“œ
    í•„í„°ë§ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    print("   [GENERATE ë…¸ë“œ] ë‹µë³€ ìƒì„± ì¤‘...")
    
    question = state["question"]
    filtered_documents = state['filtered_documents']
    classification = state.get('classification', 'general')
    
    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    if len(filtered_documents) > 0:
        context_parts = []
        for i, doc in enumerate(filtered_documents, 1):
            source = doc.metadata.get('source', 'unknown')
            disease_name = doc.metadata.get('disease_name', '')
            
            if disease_name:
                context_parts.append(f"[ì¶œì²˜ {i}: {disease_name} ({source})]")
            else:
                context_parts.append(f"[ì¶œì²˜ {i}: {source}]")
            
            context_parts.append(doc.page_content[:500])
        
        context = "\n\n---\n\n".join(context_parts)
    else:
        context = "ê´€ë ¨ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if classification == 'medical':
        system_prompt = """ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ì œê³µëœ ë¬¸ë§¥ ë‚´ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ë˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
3. ì¦ìƒì´ë‚˜ ì¹˜ë£Œë²•ì€ ìˆ˜ì˜ì‚¬ ìƒë‹´ì„ ê¶Œì¥í•˜ì„¸ìš”
4. ê¸´ê¸‰í•œ ì¦ìƒ(ì‹¬í•œ ì¶œí˜ˆ, í˜¸í¡ê³¤ë€ ë“±)ì€ ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸ì„ ê¶Œê³ í•˜ì„¸ìš”
5. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""
    
    elif classification == 'hospital':
        system_prompt = """ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ë³‘ì› ì•ˆë‚´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë³‘ì› ì°¾ê¸° ë° ìœ„ì¹˜ ì •ë³´ì— ëŒ€í•´ ë„ì›€ì„ ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³‘ì›ì„ ì¶”ì²œí•˜ì„¸ìš”
2. ìœ„ì¹˜, ì§„ë£Œì‹œê°„, ì „í™”ë²ˆí˜¸ ë“±ì„ ëª…í™•íˆ ì•ˆë‚´í•˜ì„¸ìš”
3. 24ì‹œê°„ ì‘ê¸‰ ì„œë¹„ìŠ¤ ë³‘ì› ì •ë³´ê°€ ìˆìœ¼ë©´ ê°•ì¡°í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì§ì ‘ ì „í™”ë¡œ í™•ì¸í•˜ë„ë¡ ê¶Œê³ í•˜ì„¸ìš”"""
    
    else:
        system_prompt = """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë°˜ë ¤ë™ë¬¼ ì •ë³´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë°˜ë ¤ë™ë¬¼ ê´€ë¦¬, í›ˆë ¨, ê¸°ë³¸ ì •ë³´ì— ëŒ€í•´ ë„ì›€ì„ ì£¼ì„¸ìš”.

ê·œì¹™:
1. ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
2. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ êµ¬ì¡°í™”ë˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
3. í•„ìš”ì‹œ ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", """ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")
    ])
    
    try:
        chain = prompt | generation_llm | StrOutputParser()
        answer = chain.invoke({
            "context": context,
            "question": question
        })
        print("   â†’ ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    except Exception as e:
        print(f"   âš ï¸  ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    # ì¶œì²˜ ì •ë³´ êµ¬ì„±
    sources = []
    for doc in filtered_documents:
        source_info = {
            "name": doc.metadata.get('disease_name', 'Unknown'),
            "type": doc.metadata.get('source', 'unknown'),
            "file": doc.metadata.get('file', '')
        }
        sources.append(source_info)
    
    return {
        "context": context,
        "answer": answer,
        "sources": sources
    }

# ============================================================================
# 6ï¸âƒ£ ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
# ============================================================================

def decide_to_generate(state: PetMedicalState) -> Literal["generate", "web_search"]:
    """
    ë¬¸ì„œ í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
    - ê´€ë ¨ ë¬¸ì„œ ìˆìŒ â†’ generate
    - ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ â†’ web_search
    """
    print("\n   [DECISION] ë‹¤ìŒ ë‹¨ê³„ ê²°ì • ì¤‘...")
    
    web_search_needed = state.get("web_search_needed", "No")
    classification = state.get("classification", "general")
    
    # ì˜ë£Œ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ë°”ë¡œ ë‹µë³€ ìƒì„±
    if classification != "medical":
        print("   â†’ ì˜ë£Œ ì§ˆë¬¸ ì•„ë‹˜, ë°”ë¡œ ë‹µë³€ ìƒì„±")
        return "generate"
    
    if web_search_needed == "Yes":
        print("   â†’ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì´ë™")
        return "web_search"
    else:
        print("   â†’ ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™")
        return "generate"

# ============================================================================
# 7ï¸âƒ£ StateGraph êµ¬ì„± ë° ì»´íŒŒì¼
# ============================================================================

print("ğŸ”§ LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì¤‘...\n")

# StateGraph ìƒì„±
workflow = StateGraph(PetMedicalState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify", classify_question_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("grade_documents", grade_documents_node)
workflow.add_node("web_search", web_search_node)
workflow.add_node("generate", generate_node)

# ì—£ì§€ ì¶”ê°€
# START â†’ classify â†’ retrieve â†’ grade_documents
workflow.add_edge(START, "classify")
workflow.add_edge("classify", "retrieve")
workflow.add_edge("retrieve", "grade_documents")

# ì¡°ê±´ë¶€ ì—£ì§€: grade_documents ì´í›„ ë¶„ê¸°
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "generate": "generate",
        "web_search": "web_search"
    }
)

# web_search â†’ generate â†’ END
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile()

print("âœ… ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ!\n")

# ============================================================================
# 8ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================================================

def run_pet_medical_rag(question: str):
    """
    ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ RAG ì‹¤í–‰
    """
    print("\n" + "="*70)
    print(f"ğŸ¾ ì§ˆë¬¸: {question}")
    print("="*70)
    
    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "question": question,
        "documents": [],
        "filtered_documents": [],
        "web_search_needed": "No",
        "context": "",
        "answer": "",
        "grade_results": [],
        "classification": "",
        "sources": []
    }
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    print("\nğŸ”„ CRAG ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...\n")
    
    final_state = None
    for output in app.stream(initial_state):
        for node_name, node_output in output.items():
            # ê° ë…¸ë“œ ì‹¤í–‰ì€ ìœ„ì˜ printë¬¸ìœ¼ë¡œ ì´ë¯¸ í‘œì‹œë¨
            pass
        final_state = output
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“‹ ìµœì¢… ê²°ê³¼")
    print("="*70)
    
    if "generate" in final_state and final_state["generate"]:
        answer = final_state["generate"].get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
        sources = final_state["generate"].get("sources", [])
    else:
        answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        sources = []
    
    print(f"\nğŸ’¬ ë‹µë³€:\n{answer}")
    
    if sources:
        print(f"\nğŸ“š ì°¸ê³  ì¶œì²˜ ({len(sources)}ê°œ):")
        for i, source in enumerate(sources, 1):
            print(f"   {i}. {source.get('name', 'Unknown')} ({source.get('type', 'unknown')})")
    
    print("\n" + "="*70 + "\n")

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
if __name__ == "__main__":
    test_questions = [
        "ê°•ì•„ì§€ê°€ ê³„ì† êµ¬í† ë¥¼ í•´ìš”. ë­”ê°€ ì˜ëª»ëœ ê±´ê°€ìš”?",
        "ê³ ì–‘ì´ ì„¤ì‚¬ëŠ” ì–´ë–»ê²Œ ì¹˜ë£Œí•´ì•¼ í•˜ë‚˜ìš”?",
        "ë°˜ë ¤ë™ë¬¼ ì•Œë ˆë¥´ê¸° ì¦ìƒê³¼ ì¹˜ë£Œë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ê°•ë‚¨ì—­ ê·¼ì²˜ 24ì‹œê°„ ë™ë¬¼ë³‘ì›ì„ ì°¾ì•„ì¤˜",
        "ë°˜ë ¤ê²¬ê³¼ í•¨ê»˜ ì—¬í–‰í•  ë•Œ ì£¼ì˜í•  ì ì€?"
    ]
    
    print("\nğŸ¾ ë°˜ë ¤ë™ë¬¼ ì˜ë£Œ RAG ì–´ì‹œìŠ¤í„´íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*70}")
        print(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_questions)}")
        print(f"{'='*70}")
        
        try:
            run_pet_medical_rag(question)
        except Exception as e:
            print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

