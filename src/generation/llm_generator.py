"""
LLM Generator Module
LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± ë° ì¬ì‘ì„±

ì—­í• :
  - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (ì¿¼ë¦¬ íƒ€ì…ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
  - OpenAI GPT APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„±
  - í”¼ë“œë°± ê¸°ë°˜ ì‘ë‹µ ì¬ì‘ì„±
  - í† í° ê´€ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
"""

import os
from typing import Optional, Dict
import warnings
warnings.filterwarnings("ignore")

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

if not os.environ.get('OPENAI_API_KEY'):
    raise ValueError('.env í™•ì¸í•˜ì„¸ìš”. OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤')


# RAG í”„ë¡¬í”„íŠ¸
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ ì§ˆë³‘Â·ì¦ìƒì— ëŒ€í•´ ìˆ˜ì˜í•™ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥(Context)ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

[ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ìœ í˜•]
- medical_data: ìˆ˜ì˜í•™ ì„œì  ë˜ëŠ” ë…¼ë¬¸
- qa_data: ë³´í˜¸ì-ìˆ˜ì˜ì‚¬ ìƒë‹´ ê¸°ë¡ (ìƒì• ì£¼ê¸° / ê³¼ / ì§ˆë³‘ íƒœê·¸ í¬í•¨)

[í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê·œì¹™]
1. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
2. ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ "í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. ì—¬ëŸ¬ ë¬¸ì„œ ì œê³µì‹œ, ì‹¤ì œë¡œ ë‹µë³€ì— ì‚¬ìš©í•œ ë¬¸ì„œë§Œ ì¶œì²˜ ëª…ì‹œí•˜ì„¸ìš”.
4. **ì§ˆë¬¸ì— í•©ë‹¹í•œ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”. ê±°ì§“ ì •ë³´ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”.**

[ì‘ë‹µ ê·œì¹™]
- ë³´í˜¸ìê°€ ì‘ì„±í•œ ë°˜ë ¤ê²¬ ìƒíƒœë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œë‹¤.
- ë¬¸ë§¥ì—ì„œ í™•ì¸ëœ ê°€ëŠ¥í•œ ì›ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤. 
  (ë¬¸ë§¥ì— ì—†ë‹¤ë©´ "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì“´ë‹¤)
- ì§‘ì—ì„œ ê°€ëŠ¥í•œ ì•ˆì „í•œ ê´€ë¦¬ ë°©ë²• 2~3ê°œ ì œì•ˆí•œë‹¤. 
  (ë¬¸ë§¥ì— ì—†ë‹¤ë©´ ì œì•ˆí•˜ì§€ ì•ŠëŠ”ë‹¤)
- ì–¸ì œ ë³‘ì›ì— ê°€ì•¼ í•˜ëŠ”ì§€, ì–´ë–¤ ì¦ìƒì´ ì‘ê¸‰ì¸ì§€ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
- ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•œë‹¤:
  â€¢ ì„œì  ì¶œì²˜: ì±… ì œëª© / ì €ì / ì¶œíŒì‚¬
  â€¢ QA ì¶œì²˜: ìƒì• ì£¼ê¸° / ê³¼ / ì§ˆë³‘

[ì „ì²´ í†¤]
- ê³µì†í•œ ì¡´ëŒ“ë§
- ë³´í˜¸ìë¥¼ ì•ˆì‹¬ì‹œí‚¤ë˜, í•„ìš”í•œ ë¶€ë¶„ì€ ëª…í™•í•˜ê²Œ ì•ˆë‚´í•˜ëŠ” ìˆ˜ì˜ì‚¬ ìƒë‹´ í†¤

[ì¶œë ¥ í˜•ì‹]
-ìƒíƒœ ìš”ì•½:
-ê°€ëŠ¥í•œ ì›ì¸:
-ì§‘ì—ì„œ ê´€ë¦¬ ë°©ë²•:
-ë³‘ì› ë°©ë¬¸ ì‹œê¸°:
-ì¶œì²˜(ì°¸ê³ í•œ ëª¨ë“  ë¬¸ì„œ)
"""),
    ("human", """
ë¬¸ë§¥: {context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
""")
])


# ì¿¼ë¦¬ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
REWRITE_PROMPT = PromptTemplate.from_template(
    '''
ë‹¤ìŒ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ë” ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë°”ê¿”ì£¼ì„¸ìš”.
ë³€í™˜ëœ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}
ë³€í™˜ëœ ê²€ìƒ‰ì–´:
'''
)


# RAG í”„ë¡¬í”„íŠ¸
RAG_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ ì§ˆë³‘Â·ì¦ìƒì— ëŒ€í•´ ìˆ˜ì˜í•™ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ë§¥(Context)ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

[ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ ìœ í˜•]
- medical_data: ìˆ˜ì˜í•™ ì„œì  ë˜ëŠ” ë…¼ë¬¸
- qa_data: ë³´í˜¸ì-ìˆ˜ì˜ì‚¬ ìƒë‹´ ê¸°ë¡ (ìƒì• ì£¼ê¸° / ê³¼ / ì§ˆë³‘ íƒœê·¸ í¬í•¨)

[í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê·œì¹™]
1. ë¬¸ë§¥ì— ì—†ëŠ” ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
2. ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ "í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. ì—¬ëŸ¬ ë¬¸ì„œ ì œê³µì‹œ, ì‹¤ì œë¡œ ë‹µë³€ì— ì‚¬ìš©í•œ ë¬¸ì„œë§Œ ì¶œì²˜ ëª…ì‹œí•˜ì„¸ìš”.
4. ì§ˆë¬¸ì— í•©ë‹¹í•œ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”. ê±°ì§“ ì •ë³´ë‚˜ ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”.

[ì‘ë‹µ ê·œì¹™]
- ë³´í˜¸ìê°€ ì‘ì„±í•œ ë°˜ë ¤ê²¬ ìƒíƒœë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œë‹¤.
- ë¬¸ë§¥ì—ì„œ í™•ì¸ëœ ê°€ëŠ¥í•œ ì›ì¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤. 
  (ë¬¸ë§¥ì— ì—†ë‹¤ë©´ "ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì“´ë‹¤)
- ì§‘ì—ì„œ ê°€ëŠ¥í•œ ì•ˆì „í•œ ê´€ë¦¬ ë°©ë²• 2~3ê°œ ì œì•ˆí•œë‹¤. 
  (ë¬¸ë§¥ì— ì—†ë‹¤ë©´ ì œì•ˆí•˜ì§€ ì•ŠëŠ”ë‹¤)
- ì–¸ì œ ë³‘ì›ì— ê°€ì•¼ í•˜ëŠ”ì§€, ì–´ë–¤ ì¦ìƒì´ ì‘ê¸‰ì¸ì§€ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.
- ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•œë‹¤:
  â€¢ ì„œì  ì¶œì²˜: ì±… ì œëª© / ì €ì / ì¶œíŒì‚¬
  â€¢ QA ì¶œì²˜: ìƒì• ì£¼ê¸° / ê³¼ / ì§ˆë³‘

[ì „ì²´ í†¤]
- ê³µì†í•œ ì¡´ëŒ“ë§
- ë³´í˜¸ìë¥¼ ì•ˆì‹¬ì‹œí‚¤ë˜, í•„ìš”í•œ ë¶€ë¶„ì€ ëª…í™•í•˜ê²Œ ì•ˆë‚´í•˜ëŠ” ìˆ˜ì˜ì‚¬ ìƒë‹´ í†¤

[ì¶œë ¥ í˜•ì‹]
-ìƒíƒœ ìš”ì•½:
-ê°€ëŠ¥í•œ ì›ì¸:
-ì§‘ì—ì„œ ê´€ë¦¬ ë°©ë²•:
-ë³‘ì› ë°©ë¬¸ ì‹œê¸°:
-ì¶œì²˜(ì°¸ê³ í•œ ëª¨ë“  ë¬¸ì„œ)
"""),
    ("human", """
ë¬¸ë§¥: {context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}
""")
])

# ì¿¼ë¦¬ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
REWRITE_PROMPT = PromptTemplate.from_template(
    '''
    ë‹¤ìŒ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ë” ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.
    í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë°”ê¿”ì£¼ì„¸ìš”
    ë³€í™˜ëœ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”

    ì›ë³¸ ì§ˆë¬¸: {question}
    ë³€í™˜ëœ ê²€ìƒ‰ì–´:
    ''')


def build_system_prompt(query_type: str) -> str:
    """
    ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    
    Args:
        query_type: ì¿¼ë¦¬ ë¶„ë¥˜ íƒ€ì…
        
    Returns:
        str: êµ¬ì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    """
    prompts = {
        "medical_consultation": RAG_PROMPT,
        "map_search": """ë‹¹ì‹ ì€ ì§€ì—­ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
- ì •í™•í•œ ìœ„ì¹˜, ì£¼ì†Œ, ê±°ë¦¬ ì •ë³´ ì œê³µ
- ì˜ì—… ì‹œê°„, ì—°ë½ì²˜ í¬í•¨
- ì‚¬ìš©ì ì¹œí™”ì ì¸ ì§€ë„ ì •ë³´ í˜•ì‹ ì œê³µ
- ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ ì œê³µ""",
        "general": """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
- ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€
- í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ ê¶Œìœ 
- ì •ì¤‘í•˜ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€"""
    }
    
    system_prompt = prompts.get(query_type, prompts["general"])
    print(f"âœ“ [build_system_prompt] {query_type} â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ")
    return system_prompt


def rewrite_query(query: str, model: str = "gpt-4o-mini", temperature: float = 0) -> str:
    """
    ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¬ì‘ì„±
    
    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        model: ì‚¬ìš©í•  LLM ëª¨ë¸
        temperature: ìƒì„± ì˜¨ë„
        
    Returns:
        str: ì¬ì‘ì„±ëœ ì¿¼ë¦¬
    """
    llm = ChatOpenAI(model=model, temperature=temperature)
    rewrite_chain = REWRITE_PROMPT | llm | StrOutputParser()
    
    try:
        transformed = rewrite_chain.invoke({'question': query})
        print(f"âœ“ [rewrite_query] ì›ë³¸: {query}")
        print(f"  â†’ ë³€í™˜: {transformed}")
        return transformed
    except Exception as e:
        print(f"âœ— [rewrite_query] ì‹¤íŒ¨: {e}")
        return query


def generate_response(
    query: str, 
    context: str, 
    model: str = "gpt-4o-mini", 
    temperature: float = 0
) -> str:
    """
    LLMì„ í†µí•´ ìµœì¢… ì‘ë‹µ ìƒì„±
    
    Args:
        query: ì‚¬ìš©ì ì¿¼ë¦¬
        context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
        model: ì‚¬ìš©í•  LLM ëª¨ë¸
        temperature: ìƒì„± ì˜¨ë„
        
    Returns:
        str: LLMì´ ìƒì„±í•œ ìµœì¢… ì‘ë‹µ
    """
    print(f"ğŸ¤– [generate_response] LLM ì‘ë‹µ ìƒì„± ì¤‘...")
    print(f"   - ì¿¼ë¦¬: {query[:50]}...")
    print(f"   - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì\n")
    
    try:
        llm = ChatOpenAI(model=model, temperature=temperature)
        rag_chain = RAG_PROMPT | llm | StrOutputParser()
        
        response = rag_chain.invoke({"context": context, "question": query})
        
        print(f"âœ“ [generate_response] ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response)} ë¬¸ì)\n")
        return response
        
    except Exception as e:
        print(f"âœ— [generate_response] ì‹¤íŒ¨: {e}")
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


def rewrite_response(response: str, feedback: str) -> str:
    """
    í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ì¬ì‘ì„±
    
    Args:
        response (str): ì›ë³¸ ì‘ë‹µ
        feedback (str): í‰ê°€ í”¼ë“œë°±
            ì˜ˆ: "ë„ˆë¬´ ê¸¸ì–´ìš”", "ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€ í•„ìš”", "ë” ìì„¸íˆ"
        
    Returns:
        str: ì¬ì‘ì„±ëœ ì‘ë‹µ
    
    ì²˜ë¦¬ ìˆœì„œ:
        1ï¸âƒ£  [í”¼ë“œë°± ë¶„ì„] í”¼ë“œë°± ì˜ë„ ë¶„ì„
        2ï¸âƒ£  [í”„ë¡¬í”„íŠ¸ êµ¬ì„±] ì›ë³¸ ì‘ë‹µê³¼ í”¼ë“œë°±ì„ í¬í•¨í•œ ì¬ì‘ì„± ì§€ì‹œë¬¸
        3ï¸âƒ£  [LLM í˜¸ì¶œ] ìˆ˜ì • ì§€ì‹œë¬¸ê³¼ í•¨ê»˜ LLM í˜¸ì¶œ
        4ï¸âƒ£  [ì‘ë‹µ ì¶”ì¶œ] ì¬ì‘ì„±ëœ ì‘ë‹µ ì¶”ì¶œ
        5ï¸âƒ£  [ë°˜í™˜] ì¬ì‘ì„±ëœ ì‘ë‹µ ë°˜í™˜
    
    ì¬ì‘ì„± ëª…ë ¹ ì˜ˆì‹œ:
    ```
    ì›ë³¸ ì‘ë‹µ:
    {ì›ë³¸ ì‘ë‹µ}
    
    í”¼ë“œë°±: {í”¼ë“œë°±}
    
    ì§€ì‹œ: ìœ„ í”¼ë“œë°±ì„ ê³ ë ¤í•˜ì—¬ ì‘ë‹µì„ ê°œì„ í•˜ì„¸ìš”.
    ```
    
    ì˜ˆì‹œ:
        í”¼ë“œë°±: "ë„ˆë¬´ ê¸¸ì–´ìš”"
        â†’ ì‘ë‹µ ê¸¸ì´ 50% ê°ì¶•, í•µì‹¬ë§Œ ìš”ì•½
        
        í”¼ë“œë°±: "ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€"
        â†’ ë§ˆì§€ë§‰ì— ë©´ì±… ì¡°í•­ ì¶”ê°€
    
    TODO:
        - í”¼ë“œë°± ë¶„ì„ ë° ë³€í™˜
        - ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        - LLM í˜¸ì¶œ
    """
    # TODO: ì¬ì‘ì„± ë¡œì§
    
    print(f"ğŸ”„ [rewrite_response] ì‘ë‹µ ì¬ì‘ì„± ì¤‘...")
    print(f"   í”¼ë“œë°±: {feedback}\n")
    
    # ë”ë¯¸ ì¬ì‘ì„± (ê°„ë‹¨í•œ ë³€í™˜)
    rewritten = f"""{response}

[í”¼ë“œë°± ë°˜ì˜ ìˆ˜ì •]
í”¼ë“œë°± ë‚´ìš©: {feedback}
"""
    
    print(f"âœ“ [rewrite_response] ì¬ì‘ì„± ì™„ë£Œ ({len(rewritten)} ë¬¸ì)\n")
    return rewritten


def estimate_token_count(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •
    
    Args:
        text (str): í† í°ì„ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        
    Returns:
        int: ì¶”ì •ëœ í† í° ìˆ˜
    
    ê·¼ì‚¬ê°’:
        - ì˜ì–´: 1 í† í° â‰ˆ 4 ë¬¸ì
        - í•œêµ­ì–´: 1 í† í° â‰ˆ 1.3 ë¬¸ì (ëª¨ìŒ/ììŒ ë¶„ë¦¬)
    
    ì •í™•í•œ ê³„ì‚°:
        - tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (OpenAI ê³µì‹)
    
    TODO:
        - tiktoken.encoding_for_model("gpt-4o-mini") ì‚¬ìš©
    """
    # TODO: ì •í™•í•œ í† í° ê³„ì‚° (tiktoken)
    # import tiktoken
    # encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    # tokens = encoding.encode(text)
    # return len(tokens)
    
    # ê°„ë‹¨í•œ ê·¼ì‚¬ê°’
    token_count = len(text) // 4  # í‰ê· ì ì¸ ì¶”ì •
    
    print(f"ğŸ“Š [estimate_token_count] {len(text)} ë¬¸ì â†’ ~{token_count} í† í°")
    return token_count


def truncate_context(context: str, max_length: int = 2000) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê¸°
    
    Args:
        context (str): ì›ë³¸ ì»¨í…ìŠ¤íŠ¸
        max_length (int): ìµœëŒ€ ê¸¸ì´ (ë¬¸ì ìˆ˜)
        
    Returns:
        str: ì˜ë¦° ì»¨í…ìŠ¤íŠ¸
    
    ì „ëµ:
        1. ê¸¸ì´ í™•ì¸
        2. í•„ìš”ì‹œ ëì—ì„œ ì˜ë¼ë‚´ê¸°
        3. ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ í¬í•¨
    
    TODO:
        - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        - ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (ë” ë‚˜ìŒ)
    """
    # TODO: ì»¨í…ìŠ¤íŠ¸ íŠ¸ëŸ°ì¼€ì´ì…˜ ë¡œì§
    
    if len(context) <= max_length:
        print(f"âœ“ [truncate_context] ì´ë¯¸ ìµœëŒ€ ê¸¸ì´ ì´ë‚´ ({len(context)}/{max_length})")
        return context
    
    # ëì—ì„œë¶€í„° ìë¥´ê¸° (ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€)
    truncated = context[:max_length]
    last_period = truncated.rfind('.')
    if last_period > max_length * 0.8:  # 80% ì´ìƒì˜ ë‚´ìš© ë³´ì¡´
        truncated = truncated[:last_period + 1]
    
    print(f"âœ“ [truncate_context] {len(context)} â†’ {len(truncated)} ë¬¸ì (ì œí•œ: {max_length})")
    return truncated


def calculate_token_cost(
    input_tokens: int,
    output_tokens: int,
    model: str = "gpt-4o-mini"
) -> float:
    """
    API í˜¸ì¶œì˜ ë¹„ìš© ê³„ì‚°
    
    Args:
        input_tokens (int): ì…ë ¥ í† í° ìˆ˜
        output_tokens (int): ì¶œë ¥ í† í° ìˆ˜
        model (str): ëª¨ë¸ ì´ë¦„
        
    Returns:
        float: ì˜ˆìƒ ë¹„ìš© (USD)
    
    ê°€ê²©í‘œ (ì˜ˆì‹œ):
        gpt-4o-mini:
        - ì…ë ¥: $0.15 / 1M í† í°
        - ì¶œë ¥: $0.60 / 1M í† í°
    
    TODO:
        - ìµœì‹  ê°€ê²©í‘œ ìœ ì§€
        - ëª¨ë¸ë³„ ê°€ê²© ì •ë³´
    """
    # ë”ë¯¸ ê°€ê²© ê³„ì‚°
    input_cost = (input_tokens / 1_000_000) * 0.00015
    output_cost = (output_tokens / 1_000_000) * 0.0006
    total_cost = input_cost + output_cost
    
    print(f"ğŸ’° [calculate_token_cost] ì˜ˆìƒ ë¹„ìš©: ${total_cost:.6f}")
    return total_cost


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)
    """
    
    print("\n" + "="*60)
    print("ğŸ¤– LLM Generator Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    # í…ŒìŠ¤íŠ¸ 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    print("### í…ŒìŠ¤íŠ¸ 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ###\n")
    medical_prompt = build_system_prompt("medical_consultation")
    print(f"{medical_prompt[:100]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 2: ì‘ë‹µ ìƒì„±
    print("### í…ŒìŠ¤íŠ¸ 2: LLM ì‘ë‹µ ìƒì„± ###\n")
    test_query = "ê°•ì•„ì§€ í”¼ë¶€ì—¼ì´ ë­ì˜ˆìš”?"
    test_context = "[ê²€ìƒ‰ ê²°ê³¼] í”¼ë¶€ì—¼ ê´€ë ¨ ì˜ë£Œ ì •ë³´..."
    response = generate_response(test_query, test_context)
    print(f"{response[:200]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 3: ì‘ë‹µ ì¬ì‘ì„±
    print("### í…ŒìŠ¤íŠ¸ 3: ì‘ë‹µ ì¬ì‘ì„± ###\n")
    feedback = "ë„ˆë¬´ ê¸¸ì–´ìš”, ìš”ì•½í•´ì£¼ì„¸ìš”"
    rewritten = rewrite_response(response, feedback)
    print(f"{rewritten[:200]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 4: í† í° ì¶”ì •
    print("### í…ŒìŠ¤íŠ¸ 4: í† í° ê³„ì‚° ###\n")
    tokens = estimate_token_count(response)
    print(f"ì˜ˆìƒ í† í°: {tokens}\n")
    
    # í…ŒìŠ¤íŠ¸ 5: ë¹„ìš© ê³„ì‚°
    print("### í…ŒìŠ¤íŠ¸ 5: ë¹„ìš© ê³„ì‚° ###\n")
    calculate_token_cost(100, 300)
    print()
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)

