"""
Evaluation Controller Module
ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë° íë¦„ ì œì–´

ì—­í• :
  - ë‹¤ì°¨ì› ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ì •í™•ë„, ëª…í™•ì„±, ì™„ì „ì„±, ì•ˆì „ì„±)
  - í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
  - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¡œê¹…
  - ì‘ë‹µ ê°œì„  í”¼ë“œë°± ìƒì„±
"""

from typing import Dict, Literal, Tuple
from datetime import datetime


def evaluate_response(response: str) -> Dict[str, any]:
    """ì‘ë‹µì„ 4ê°œ ì°¨ì›ìœ¼ë¡œ í‰ê°€"""
    print(f"âš–ï¸  [evaluate_response] ì‘ë‹µ í‰ê°€ ì¤‘...\n")
    
    scores = {
        'accuracy': check_accuracy(response),
        'clarity': check_clarity(response),
        'completeness': check_completeness(response),
        'safety': check_safety_guidelines(response)['passed']
    }
    
    average_score = sum(scores.values()) / len(scores)
    passed = average_score >= 0.75
    feedback = generate_feedback(scores, response)
    
    evaluation = {
        'pass': passed,
        'scores': scores,
        'average_score': average_score,
        'feedback': feedback,
        'reason': 'í‰ê°€ ì™„ë£Œ'
    }
    
    print(f"âœ“ í‰ê°€ ì™„ë£Œ")
    print(f"  - ì •í™•ë„: {scores['accuracy']:.0%}")
    print(f"  - ëª…í™•ì„±: {scores['clarity']:.0%}")
    print(f"  - ì™„ì „ì„±: {scores['completeness']:.0%}")
    print(f"  - ì•ˆì „ì„±: {scores['safety']:.0%}")
    print(f"  - í‰ê· : {average_score:.0%}")
    print(f"  - íŒì •: {'âœ… í†µê³¼' if passed else 'ğŸ”„ ì¬ì‘ì„± í•„ìš”'}\n")
    
    return evaluation


def check_accuracy(response: str) -> float:
    """ì‘ë‹µì˜ ì •í™•ë„ í‰ê°€"""
    if len(response) > 100:
        accuracy = 0.85
    else:
        accuracy = 0.65
    return accuracy


def check_clarity(response: str) -> float:
    """ì‘ë‹µì˜ ëª…í™•ì„± í‰ê°€"""
    if len(response) > 50:
        clarity = 0.80
    else:
        clarity = 0.70
    return clarity


def check_completeness(response: str) -> float:
    """ì‘ë‹µì˜ ì™„ì „ì„± í‰ê°€"""
    if len(response) > 200:
        completeness = 0.85
    else:
        completeness = 0.70
    return completeness


def check_safety_guidelines(response: str) -> Dict[str, any]:
    """ì‘ë‹µì´ ì•ˆì „ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ í‰ê°€"""
    print(f"ğŸ›¡ï¸  [check_safety_guidelines] ì•ˆì „ì„± ê²€ì‚¬\n")
    
    has_disclaimer = any(kw in response for kw in ['ë©´ì±…', 'ìˆ˜ì˜ì‚¬ ì§„ë£Œ', 'ì „ë¬¸ê°€ ìƒë‹´'])
    has_emergency_warning = any(kw in response for kw in ['ì‘ê¸‰', 'ì¦‰ì‹œ', '119', 'ë³‘ì› ë°©ë¬¸'])
    
    issues = []
    if not has_disclaimer:
        issues.append("ì˜ë£Œ ë©´ì±… ì¡°í•­ ëˆ„ë½")
    if not has_emergency_warning and 'ì¦ìƒ' in response:
        issues.append("ì‘ê¸‰ ê²½ê³  í‘œì‹œ ë¶€ì¡±")
    
    safety_score = 0.9 if has_disclaimer else 0.7
    
    safety_result = {
        'passed': safety_score,
        'has_disclaimer': has_disclaimer,
        'has_emergency_warning': has_emergency_warning,
        'issues': issues
    }
    
    print(f"  - ë©´ì±… ì¡°í•­: {'âœ“' if has_disclaimer else 'âœ—'}")
    print(f"  - ì‘ê¸‰ ê²½ê³ : {'âœ“' if has_emergency_warning else 'âœ—'}")
    print(f"  - ì•ˆì „ë„: {safety_score:.0%}")
    print(f"  - ë¬¸ì œ: {len(issues)}ê°œ\n")
    
    return safety_result


def determine_next_action(
    response: str,
    evaluation: Dict[str, any]
) -> Literal["accept", "rewrite", "escalate"]:
    """í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ë‹¤ìŒ ì•¡ì…˜ ê²°ì •"""
    avg_score = evaluation.get('average_score', 0)
    
    if avg_score >= 0.75:
        action = "accept"
    elif avg_score >= 0.50:
        action = "rewrite"
    else:
        action = "escalate"
    
    print(f"ğŸ¯ [determine_next_action] ì ìˆ˜: {avg_score:.0%} â†’ ì•¡ì…˜: {action}")
    
    return action


def generate_feedback(scores: Dict[str, float], response: str) -> str:
    """í‰ê°€ ì ìˆ˜ ê¸°ë°˜ ê°œì„  í”¼ë“œë°± ìƒì„±"""
    feedback_list = []
    
    if scores.get('accuracy', 1.0) < 0.7:
        feedback_list.append("ì •í™•ë„ ê°œì„  í•„ìš”")
    if scores.get('clarity', 1.0) < 0.7:
        feedback_list.append("í‘œí˜„ì´ ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤")
    if scores.get('completeness', 1.0) < 0.7:
        feedback_list.append("ë” ìì„¸í•œ ì„¤ëª… í•„ìš”")
    if scores.get('safety', 1.0) < 0.7:
        feedback_list.append("ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€ í•„ìš”")
    
    if len(response) > 500:
        feedback_list.append("ë‹µë³€ì´ ê¸¸ì–´ìš”")
    
    feedback = " | ".join(feedback_list) if feedback_list else "ì‘ë‹µì´ ìš°ìˆ˜í•©ë‹ˆë‹¤"
    
    return feedback


def collect_evaluation_metrics(
    response: str,
    evaluation: Dict[str, any],
    generation_time: float,
    rewrite_count: int
) -> Dict[str, any]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° í†µê³„ ìƒì„±"""
    
    metrics = {
        'response_length': len(response),
        'generation_time': generation_time,
        'rewrite_count': rewrite_count,
        'evaluation_scores': evaluation.get('scores', {}),
        'average_score': evaluation.get('average_score', 0),
        'passed_evaluation': evaluation.get('pass', False),
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"ğŸ“Š [collect_evaluation_metrics] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"  - ì‘ë‹µ ê¸¸ì´: {metrics['response_length']} ë¬¸ì")
    print(f"  - ìƒì„± ì‹œê°„: {metrics['generation_time']:.2f}ì´ˆ")
    print(f"  - ì¬ì‘ì„±: {metrics['rewrite_count']}íšŒ")
    print(f"  - í‰ê°€ ì ìˆ˜: {metrics['average_score']:.0%}")
    
    return metrics


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)"""
    
    print("\n" + "="*60)
    print("âš–ï¸  Evaluation Controller Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    test_response = """ê°•ì•„ì§€ í”¼ë¶€ì—¼ì€ í”¼ë¶€ í‘œë©´ì˜ ì—¼ì¦ì…ë‹ˆë‹¤.
    
ì£¼ìš” ì¦ìƒ:
- ê°€ë ¤ì›€ì¦
- í”¼ë¶€ ë°œì 

ì¹˜ë£Œ ë°©ë²•:
- ì•½ë¬¼ ì¹˜ë£Œ
- í”¼ë¶€ ê´€ë¦¬

âš ï¸  ì´ëŠ” ì¼ë°˜ ì •ë³´ì´ë©° ì „ë¬¸ ìˆ˜ì˜ì‚¬ ì§„ë£Œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."""
    
    print("### í…ŒìŠ¤íŠ¸ 1: í‰ê°€ ###\n")
    evaluation = evaluate_response(test_response)
    print(f"í‰ê°€ ê²°ê³¼: {'âœ… í†µê³¼' if evaluation['pass'] else 'ğŸ”„ ì¬ì‘ì„±'}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 2: ì•ˆì „ì„± ê²€ì‚¬ ###\n")
    safety = check_safety_guidelines(test_response)
    print(f"ì•ˆì „ì„± ì ìˆ˜: {safety['passed']:.0%}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 3: ë‹¤ìŒ ì•¡ì…˜ ê²°ì • ###\n")
    action = determine_next_action(test_response, evaluation)
    print(f"ê²°ì •: {action}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 4: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ###\n")
    metrics = collect_evaluation_metrics(test_response, evaluation, 2.5, 1)
    print()
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)

