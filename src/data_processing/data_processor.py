"""
Data Processor Module
ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ì²˜ë¦¬

ì—­í• :
  - ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ë¡œë”© (PDF, TXT, JSON ë“±)
  - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ê·œí™”
  - ë¬¸ì„œ ì²­í‚¹ (ê³ ì • í¬ê¸° ë˜ëŠ” ì˜ë¯¸ ê¸°ë°˜)
  - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ê´€ë¦¬
"""

import os
import json
import glob
from typing import List, Dict, Optional, Tuple
import re
import warnings
warnings.filterwarnings("ignore")

from langchain_core.documents import Document
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter


def load_medical_data(paths: List[str]) -> List[Document]:
    """
    ì˜í•™ì§€ì‹ ë°ì´í„° ë¡œë“œ (ë§ë­‰ì¹˜ ë°ì´í„°)
    
    Args:
        paths: ë§ë­‰ì¹˜ ë°ì´í„° í´ë” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Document]: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    docs = []
    
    for path in paths:
        print(f"ì²˜ë¦¬ ì¤‘ì¸ ê²½ë¡œ: {path}")
        
        for file_path in glob.glob(os.path.join(path, "**", "*.json"), recursive=True):
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                
                disease = data.get("disease", "") or ""
                
                # ë¬¸ì„œ ë‚´ìš©: ì§ˆë³‘ ì •ë³´
                page_content = disease
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                meta = {
                    "title": data.get("title", ""),
                    "author": data.get("author", None),
                    "publisher": data.get("publisher", None),
                    "department": data.get("department", None),
                    "source_type": "medical_data",
                    "source_path": path,
                }
                
                docs.append(Document(page_content=page_content, metadata=meta))
            except Exception as e:
                print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                continue
    
    print(f"ì´ {len(docs)}ê°œ ì˜í•™ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return docs


def load_qa_data(paths: List[str]) -> List[Document]:
    """
    ì§ˆì˜ì‘ë‹µ ë°ì´í„° ë¡œë“œ
    
    Args:
        paths: ì§ˆì˜ì‘ë‹µ ë°ì´í„° í´ë” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Document]: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    docs_qa = []
    
    for path_qa in paths:
        print(f"ì²˜ë¦¬ ì¤‘ì¸ ê²½ë¡œ: {path_qa}")
        
        for file_path in glob.glob(os.path.join(path_qa, "**", "*.json"), recursive=True):
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                
                # metaì™€ qa ì¶”ì¶œ
                meta_info = data.get("meta", {})
                qa_info = data.get("qa", {})
                
                # page_content: ì§ˆë¬¸ + ë‹µë³€ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                question = qa_info.get("input", "")
                answer = qa_info.get("output", "")
                
                # Q&A í˜•íƒœë¡œ êµ¬ì„± (ê²€ìƒ‰ ì‹œ ë” íš¨ê³¼ì )
                page_content = f"Q: {question}\n\nA: {answer}"
                
                # metadata: ë©”íƒ€ì •ë³´ + QA ê´€ë ¨ ì •ë³´
                metadata = {
                    "lifeCycle": meta_info.get("lifeCycle", ""),
                    "department": meta_info.get("department", ""),
                    "disease": meta_info.get("disease", ""),
                    "question": question,
                    "answer": answer,
                    "source_type": "qa_data",
                    "source_path": path_qa
                }
                
                docs_qa.append(Document(page_content=page_content, metadata=metadata))
            except Exception as e:
                print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                continue
    
    print(f"ì´ {len(docs_qa)}ê°œ QA ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return docs_qa


def preprocess_document(file_path: str) -> List[str]:
    """
    íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        file_path (str): ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: "data/disease/disease_001.json")
        
    Returns:
        List[str]: ì „ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ëŠ” batch_preprocess_documentsë¥¼ í†µí•´ ì²˜ë¦¬
    results = batch_preprocess_documents([file_path])
    if results:
        return [chunk.page_content for chunk in results]
    return []


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì •ì œ
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì •ì œëœ í…ìŠ¤íŠ¸
        
    ì²˜ë¦¬:
        - ì—¬ëŸ¬ ê°œì˜ ê³µë°± ì œê±° â†’ ë‹¨ì¼ ê³µë°±
        - íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        - ë¶ˆí•„ìš”í•œ ë¼ì¸ ë¸Œë ˆì´í¬ ì œê±°
        - HTML íƒœê·¸ ì œê±° (if present)
        - ì¸ì½”ë”© ì •ìƒí™” (UTF-8)
    
    TODO:
        - ì •ê·œì‹ ê¸°ë°˜ ì „ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
        - ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì œê±° (ì„ íƒ)
    """
    # TODO: ì •ê·œì‹ ê¸°ë°˜ ì „ì²˜ë¦¬
    # - re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°± â†’ ë‹¨ì¼ ê³µë°±
    # - re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
    
    cleaned = text.strip()
    print(f"âœ“ [clean_text] í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ: {len(text)} â†’ {len(cleaned)} ë¬¸ì")
    return cleaned


def chunk_text(
    text: str,
    chunk_size: int = 500,
    overlap: int = 50
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” í¬ê¸°ë¡œ ë¶„í• 
    
    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size (int): ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 500)
        overlap (int): ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 50)
        
    Returns:
        List[str]: ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
    ì²­í‚¹ ì „ëµ:
        1. Fixed-Size Chunking (ê³ ì • í¬ê¸°):
           - í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• 
           - ê°„ë‹¨í•˜ì§€ë§Œ ì˜ë¯¸ ê²½ê³„ ë¬´ì‹œ ê°€ëŠ¥
        
        2. Semantic Chunking (ì˜ë¯¸ ê¸°ë°˜):
           - ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
           - ë” ë‚˜ìŒ (í–¥í›„ êµ¬í˜„)
        
    ì˜ˆì‹œ:
        í…ìŠ¤íŠ¸: "ë¬¸ì¥1. ë¬¸ì¥2. ë¬¸ì¥3. ..."
        chunk_size=20, overlap=5
        
        ê²°ê³¼:
        ["ë¬¸ì¥1. ë¬¸ì¥2", "ì¥2. ë¬¸ì¥3", ...]
    
    TODO:
        - ê³ ì • í¬ê¸° ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        - ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„) ì¶”ê°€ êµ¬í˜„
        - ë¹ˆ ì²­í¬ í•„í„°ë§
    """
    # TODO: ì²­í‚¹ ë¡œì§
    # - í…ìŠ¤íŠ¸ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ë¶„í• 
    # - overlapë§Œí¼ ê²¹ì¹˜ë„ë¡ ì¡°ì •
    
    # ë”ë¯¸ ë°ì´í„°
    num_chunks = (len(text) // (chunk_size - overlap)) + 1
    chunks = [f"[ì²­í¬ {i}] {text[i*(chunk_size-overlap):i*(chunk_size-overlap)+chunk_size]}"
              for i in range(max(1, num_chunks))]
    
    print(f"âœ“ [chunk_text] {len(text)} ë¬¸ì â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    return chunks


def extract_metadata(file_path: str, text: str) -> Dict[str, str]:
    """
    ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    
    Args:
        file_path (str): ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
        text (str): ë¬¸ì„œ í…ìŠ¤íŠ¸
        
    Returns:
        Dict[str, str]: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            {
                'source': 'íŒŒì¼ ê²½ë¡œ',
                'title': 'ë¬¸ì„œ ì œëª©',
                'date_created': 'ìƒì„± ë‚ ì§œ',
                'content_length': 'í…ìŠ¤íŠ¸ ê¸¸ì´',
                'language': 'ì–¸ì–´'
            }
    
    TODO:
        - íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        - ë¬¸ì„œ ë‚´ ì œëª© ì¶”ì¶œ (ì •ê·œì‹ ë˜ëŠ” NLP)
        - ì–¸ì–´ ê°ì§€
    """
    # TODO: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë¡œì§
    
    metadata = {
        'source': file_path,
        'title': file_path.split('/')[-1].split('.')[0],
        'content_length': len(text),
        'language': 'ko'
    }
    
    print(f"âœ“ [extract_metadata] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {metadata}")
    return metadata


def chunk_documents(docs: List[Document]) -> List[Document]:
    """
    ë¬¸ì„œë“¤ì„ ì²­í‚¹ ì²˜ë¦¬
    
    Args:
        docs: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Document]: ì²­í‚¹ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    # ë°ì´í„° íƒ€ì…ë³„ splitter ì •ì˜
    splitter_map = {
        # ì˜í•™ ë°ì´í„° (ê¸´ ì„¤ëª…ë¬¸)
        "medical_data": RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            separators=['\n\n', '\n', '.', '!', '?', ',', ' ', '']
        ),
        
        # QA ë°ì´í„° (ì§ˆë¬¸-ë‹µë³€ ìŒ)
        "qa_data": RecursiveCharacterTextSplitter(
            chunk_size=800,  # QAëŠ” ë” í° ì²­í¬ë¡œ
            chunk_overlap=50,
            separators=['\n\nA:', 'Q:', '\n\n', '\n', '.', ' ', '']
        )
    }
    
    # ê¸°ë³¸ splitter (ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
    default_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=['\n\n', '\n', '.', ',', ' ', '']
    )
    
    chunked_docs = []
    
    print(f"\nì²­í‚¹ ëŒ€ìƒ ì›ë³¸ Document ìˆ˜: {len(docs)}ê°œ")
    
    # ê° ë¬¸ì„œì˜ source_typeì— ë”°ë¼ ë‹¤ë¥¸ splitter ì ìš©
    for doc in docs:
        source_type = doc.metadata.get("source_type", "")
        
        # ë°ì´í„° íƒ€ì…ì— ë§ëŠ” splitter ì„ íƒ
        if source_type == "medical_data":
            splitter = splitter_map["medical_data"]
        elif source_type == "qa_data":
            splitter = splitter_map["qa_data"]
        else:
            splitter = default_splitter
        
        # ì²­í‚¹ ì‹¤í–‰
        chunks = splitter.split_documents([doc])
        
        # ì²­í‚¹ëœ ë¬¸ì„œë“¤ì— ì›ë³¸ ë©”íƒ€ë°ì´í„° ë³´ì¡´ + ì²­í‚¹ ì •ë³´ ì¶”ê°€
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_method": source_type
            })
        
        chunked_docs.extend(chunks)
    
    print(f"ìµœì¢… ì²­í‚¹ ê²°ê³¼: {len(chunked_docs)}ê°œ Document")
    return chunked_docs


def batch_preprocess_documents(
    file_paths: List[str],
    chunk_size: int = 500
) -> List[Document]:
    """
    ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì „ì²˜ë¦¬
    
    Args:
        file_paths (List[str]): ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (í´ë” ê²½ë¡œ)
        chunk_size (int): ì²­í¬ í¬ê¸°
        
    Returns:
        List[Document]: ì²­í‚¹ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸ”„ [batch_preprocess_documents] ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘\n")
    
    # ì˜í•™ ë°ì´í„° ê²½ë¡œì™€ QA ë°ì´í„° ê²½ë¡œ ë¶„ë¦¬
    medical_paths = [p for p in file_paths if "ë§ë­‰ì¹˜" in p or "medical" in p.lower()]
    qa_paths = [p for p in file_paths if "ì§ˆì˜ì‘ë‹µ" in p or "qa" in p.lower()]
    
    # 1. ì˜í•™ì§€ì‹ ë°ì´í„° ë¡œë“œ
    print("\n" + "=" * 30)
    print("ì˜í•™ì§€ì‹ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
    print("=" * 30)
    
    docs = []
    if medical_paths:
        docs = load_medical_data(medical_paths)
        if docs:
            print(f"ìƒ˜í”Œ ì˜í•™ ë¬¸ì„œ:\n{docs[0].page_content[:300]}")
            print(f"ë©”íƒ€ë°ì´í„°: {docs[0].metadata}")
    
    # 2. ì§ˆì˜ì‘ë‹µ ë°ì´í„° ë¡œë“œ
    print("\n" + "=" * 30)
    print("ì§ˆì˜ì‘ë‹µ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
    print("=" * 30)
    
    if qa_paths:
        docs_qa = load_qa_data(qa_paths)
        if docs_qa:
            print(f"ìƒ˜í”Œ QA ë¬¸ì„œ:\n{docs_qa[0].page_content[:300]}")
            print(f"ë©”íƒ€ë°ì´í„°: {docs_qa[0].metadata}")
            docs.extend(docs_qa)
    
    print(f"\nìµœì¢… ë¬¸ì„œ ê°œìˆ˜: {len(docs)}ê°œ")
    
    # 3. ì²­í‚¹
    print("\n" + "=" * 30)
    print("ë¬¸ì„œ ì²­í‚¹ ì²˜ë¦¬")
    print("=" * 30)
    
    chunked_docs = chunk_documents(docs)
    
    print(f"\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(chunked_docs)}ê°œ ì²­í¬ ìƒì„±\n")
    return chunked_docs


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)
    """
    
    print("\n" + "="*60)
    print("ğŸ“„ Data Processor Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    # í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ì „ì²˜ë¦¬
    print("### í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ì „ì²˜ë¦¬ ###\n")
    test_file = "data/disease/disease_001.json"
    chunks = preprocess_document(test_file)
    print(f"âœ“ {len(chunks)}ê°œ ì²­í¬ ìƒì„±\n")
    
    # í…ŒìŠ¤íŠ¸ 2: í…ìŠ¤íŠ¸ ì •ê·œí™”
    print("### í…ŒìŠ¤íŠ¸ 2: í…ìŠ¤íŠ¸ ì •ê·œí™” ###\n")
    sample_text = "  ì—¬ëŸ¬    ê³µë°±ì´   ìˆëŠ”    í…ìŠ¤íŠ¸  ì…ë‹ˆë‹¤.  "
    cleaned = clean_text(sample_text)
    print(f"ì›ë³¸: '{sample_text}'")
    print(f"ì •ì œ: '{cleaned}'\n")
    
    # í…ŒìŠ¤íŠ¸ 3: ë°°ì¹˜ ì²˜ë¦¬
    print("### í…ŒìŠ¤íŠ¸ 3: ë°°ì¹˜ ì²˜ë¦¬ ###\n")
    test_files = [
        "data/disease/disease_001.json",
        "data/disease/disease_002.json"
    ]
    batch_results = batch_preprocess_documents(test_files)
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)

