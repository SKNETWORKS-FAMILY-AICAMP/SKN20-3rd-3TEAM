"""
Data Processor Module
ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ì²­í‚¹ ì²˜ë¦¬

ì—­í• :
  - ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œ ë¡œë”© (PDF, TXT, JSON ë“±)
  - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ê·œí™”
  - ë¬¸ì„œ ì²­í‚¹ (ê³ ì • í¬ê¸° ë˜ëŠ” ì˜ë¯¸ ê¸°ë°˜)
  - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ê´€ë¦¬
"""

from typing import List, Dict, Optional, Tuple
import re


def preprocess_document(file_path: str) -> List[str]:
    """
    íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì „ì²˜ë¦¬í•˜ê³  ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        file_path (str): ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ (ì˜ˆ: "data/disease/disease_001.json")
        
    Returns:
        List[str]: ì „ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            ê° ì²­í¬ëŠ” ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• ëœ í…ìŠ¤íŠ¸
            ì˜ˆ: ["ì²­í¬1 í…ìŠ¤íŠ¸...", "ì²­í¬2 í…ìŠ¤íŠ¸...", ...]
        
    ì²˜ë¦¬ ìˆœì„œ:
        1ï¸âƒ£  [íŒŒì¼ ë¡œë“œ] íŒŒì¼ í˜•ì‹ ê°ì§€ ë° ë¡œë“œ (JSON/TXT/PDF)
        2ï¸âƒ£  [í…ìŠ¤íŠ¸ ì¶”ì¶œ] JSON â†’ dict â†’ í•„ë“œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        3ï¸âƒ£  [ì „ì²˜ë¦¬] ë¶ˆí•„ìš”í•œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        4ï¸âƒ£  [ì²­í‚¹] ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„í•  (chunk_size=500, overlap=50)
        5ï¸âƒ£  [ê²€ì¦] ë¹ˆ ì²­í¬ ì œê±°, ìµœì†Œ ê¸¸ì´ í™•ì¸
        
    ì˜ˆì‹œ:
        ì…ë ¥: "data/disease/disease_001.json"
        
        íŒŒì¼ ë‚´ìš©:
        {
            "disease_name": "ê°•ì•„ì§€ í”¼ë¶€ì—¼",
            "symptoms": "ê°€ë ¤ì›€ì¦, í„¸ ì†ì‹¤",
            "treatment": "ì•½ë¬¼ ì¹˜ë£Œ..."
        }
        
        ì¶œë ¥:
        [
            "ê°•ì•„ì§€ í”¼ë¶€ì—¼ ì¦ìƒ: ê°€ë ¤ì›€ì¦, í„¸ ì†ì‹¤",
            "ê°•ì•„ì§€ í”¼ë¶€ì—¼ ì¹˜ë£Œ ë°©ë²•: ì•½ë¬¼ ì¹˜ë£Œ..."
        ]
    
    TODO:
        - íŒŒì¼ í˜•ì‹ë³„ ë¡œë” êµ¬í˜„ (load_json, load_txt, load_pdf)
        - í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ êµ¬í˜„
        - ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ë˜ëŠ” ê³ ì • í¬ê¸° ì²­í‚¹ êµ¬í˜„
        - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì¶œì²˜ ë“±)
    """
    # íŒŒì¼ í˜•ì‹ ê°ì§€
    file_ext = file_path.split('.')[-1].lower()
    
    # ë”ë¯¸ ë°ì´í„°: ì „ì²˜ë¦¬ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    chunks = [
        f"[ì²­í¬ 1] ë¬¸ì„œ: {file_path}\në¬¸ì„œ ë‚´ìš© ì²­í¬ 1: ì˜ë£Œ ì •ë³´ ê´€ë ¨ í…ìŠ¤íŠ¸...",
        f"[ì²­í¬ 2] ë¬¸ì„œ: {file_path}\në¬¸ì„œ ë‚´ìš© ì²­í¬ 2: ì¹˜ë£Œ ë°©ë²• ê´€ë ¨ í…ìŠ¤íŠ¸..."
    ]
    
    print(f"ğŸ“„ [preprocess_document] {file_path} ì²˜ë¦¬ ì™„ë£Œ â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    return chunks


def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ê·œí™” ë° ì •ì œ
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        str: ì •ì œëœ í…ìŠ¤íŠ¸
        
    ì²˜ë¦¬:
        - ì—¬ëŸ¬ ê°œì˜ ê³µë°± ì œê±° â†’ ë‹¨ì¼ ê³µë°±
        - íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        - ë¶ˆí•„ìš”í•œ ë¼ì¸ ë¸Œë ˆì´í¬ ì œê±°
        - HTML íƒœê·¸ ì œê±° (if present)
        - ì¸ì½”ë”© ì •ìƒí™” (UTF-8)
    
    TODO:
        - ì •ê·œì‹ ê¸°ë°˜ ì „ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
        - ì–¸ì–´ë³„ ë¶ˆìš©ì–´ ì œê±° (ì„ íƒ)
    """
    # TODO: ì •ê·œì‹ ê¸°ë°˜ ì „ì²˜ë¦¬
    # - re.sub(r'\s+', ' ', text)  # ì—¬ëŸ¬ ê³µë°± â†’ ë‹¨ì¼ ê³µë°±
    # - re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
    
    cleaned = text.strip()
    print(f"âœ“ [clean_text] í…ìŠ¤íŠ¸ ì •ì œ ì™„ë£Œ: {len(text)} â†’ {len(cleaned)} ë¬¸ì")
    return cleaned


def chunk_text(
    text: str,
    chunk_size: int = 500,
    overlap: int = 50
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” í¬ê¸°ë¡œ ë¶„í• 
    
    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size (int): ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 500)
        overlap (int): ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ê¸°ë³¸ê°’: 50)
        
    Returns:
        List[str]: ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
    ì²­í‚¹ ì „ëµ:
        1. Fixed-Size Chunking (ê³ ì • í¬ê¸°):
           - í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë¶„í• 
           - ê°„ë‹¨í•˜ì§€ë§Œ ì˜ë¯¸ ê²½ê³„ ë¬´ì‹œ ê°€ëŠ¥
        
        2. Semantic Chunking (ì˜ë¯¸ ê¸°ë°˜):
           - ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
           - ë” ë‚˜ìŒ (í–¥í›„ êµ¬í˜„)
        
    ì˜ˆì‹œ:
        í…ìŠ¤íŠ¸: "ë¬¸ì¥1. ë¬¸ì¥2. ë¬¸ì¥3. ..."
        chunk_size=20, overlap=5
        
        ê²°ê³¼:
        ["ë¬¸ì¥1. ë¬¸ì¥2", "ì¥2. ë¬¸ì¥3", ...]
    
    TODO:
        - ê³ ì • í¬ê¸° ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        - ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ (ë¬¸ì¥/ë¬¸ë‹¨ ë‹¨ìœ„) ì¶”ê°€ êµ¬í˜„
        - ë¹ˆ ì²­í¬ í•„í„°ë§
    """
    # TODO: ì²­í‚¹ ë¡œì§
    # - í…ìŠ¤íŠ¸ë¥¼ chunk_size ë‹¨ìœ„ë¡œ ë¶„í• 
    # - overlapë§Œí¼ ê²¹ì¹˜ë„ë¡ ì¡°ì •
    
    # ë”ë¯¸ ë°ì´í„°
    num_chunks = (len(text) // (chunk_size - overlap)) + 1
    chunks = [f"[ì²­í¬ {i}] {text[i*(chunk_size-overlap):i*(chunk_size-overlap)+chunk_size]}"
              for i in range(max(1, num_chunks))]
    
    print(f"âœ“ [chunk_text] {len(text)} ë¬¸ì â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    return chunks


def extract_metadata(file_path: str, text: str) -> Dict[str, str]:
    """
    ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    
    Args:
        file_path (str): ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
        text (str): ë¬¸ì„œ í…ìŠ¤íŠ¸
        
    Returns:
        Dict[str, str]: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            {
                'source': 'íŒŒì¼ ê²½ë¡œ',
                'title': 'ë¬¸ì„œ ì œëª©',
                'date_created': 'ìƒì„± ë‚ ì§œ',
                'content_length': 'í…ìŠ¤íŠ¸ ê¸¸ì´',
                'language': 'ì–¸ì–´'
            }
    
    TODO:
        - íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        - ë¬¸ì„œ ë‚´ ì œëª© ì¶”ì¶œ (ì •ê·œì‹ ë˜ëŠ” NLP)
        - ì–¸ì–´ ê°ì§€
    """
    # TODO: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë¡œì§
    
    metadata = {
        'source': file_path,
        'title': file_path.split('/')[-1].split('.')[0],
        'content_length': len(text),
        'language': 'ko'
    }
    
    print(f"âœ“ [extract_metadata] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {metadata}")
    return metadata


def batch_preprocess_documents(
    file_paths: List[str],
    chunk_size: int = 500
) -> List[Dict[str, any]]:
    """
    ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì „ì²˜ë¦¬
    
    Args:
        file_paths (List[str]): ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        chunk_size (int): ì²­í¬ í¬ê¸°
        
    Returns:
        List[Dict[str, any]]: ê° ë¬¸ì„œë³„ ì²˜ë¦¬ ê²°ê³¼
            [
                {
                    'file_path': 'path/to/file',
                    'chunks': ['ì²­í¬1', 'ì²­í¬2', ...],
                    'chunk_count': 2,
                    'metadata': {...}
                },
                ...
            ]
    
    TODO:
        - ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
        - ì—ëŸ¬ ì²˜ë¦¬ (íŒŒì¼ ì—†ìŒ ë“±)
        - ì§„í–‰ë¥  í‘œì‹œ
    """
    # TODO: ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§
    
    results = []
    
    print(f"\nğŸ”„ [batch_preprocess_documents] {len(file_paths)}ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘\n")
    
    for idx, file_path in enumerate(file_paths, 1):
        print(f"  [{idx}/{len(file_paths)}] ì²˜ë¦¬ ì¤‘: {file_path}")
        
        # ê° íŒŒì¼ ì²˜ë¦¬
        chunks = preprocess_document(file_path)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        text = '\n'.join(chunks)
        metadata = extract_metadata(file_path, text)
        
        result = {
            'file_path': file_path,
            'chunks': chunks,
            'chunk_count': len(chunks),
            'metadata': metadata
        }
        
        results.append(result)
    
    print(f"\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì´ {sum(r['chunk_count'] for r in results)}ê°œ ì²­í¬ ìƒì„±\n")
    return results


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)
    """
    
    print("\n" + "="*60)
    print("ğŸ“„ Data Processor Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    # í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ì „ì²˜ë¦¬
    print("### í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ë¬¸ì„œ ì „ì²˜ë¦¬ ###\n")
    test_file = "data/disease/disease_001.json"
    chunks = preprocess_document(test_file)
    print(f"âœ“ {len(chunks)}ê°œ ì²­í¬ ìƒì„±\n")
    
    # í…ŒìŠ¤íŠ¸ 2: í…ìŠ¤íŠ¸ ì •ê·œí™”
    print("### í…ŒìŠ¤íŠ¸ 2: í…ìŠ¤íŠ¸ ì •ê·œí™” ###\n")
    sample_text = "  ì—¬ëŸ¬    ê³µë°±ì´   ìˆëŠ”    í…ìŠ¤íŠ¸  ì…ë‹ˆë‹¤.  "
    cleaned = clean_text(sample_text)
    print(f"ì›ë³¸: '{sample_text}'")
    print(f"ì •ì œ: '{cleaned}'\n")
    
    # í…ŒìŠ¤íŠ¸ 3: ë°°ì¹˜ ì²˜ë¦¬
    print("### í…ŒìŠ¤íŠ¸ 3: ë°°ì¹˜ ì²˜ë¦¬ ###\n")
    test_files = [
        "data/disease/disease_001.json",
        "data/disease/disease_002.json"
    ]
    batch_results = batch_preprocess_documents(test_files)
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)

