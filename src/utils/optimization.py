"""
RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™” ë° ì•ˆì •í™” ëª¨ë“ˆ
- í‚¤ì›Œë“œ ì¶”ì¶œ (Query Re-writing)
- ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)
- ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥/ë¡œë“œ
- ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™”
- ê²½ë¡œ ê´€ë¦¬
"""

import os
import pickle
import re
from typing import List, Optional, Dict, Any
from pathlib import Path

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# KoNLPy ë¶ˆìš©ì–´ ì œê±°ìš© (ì„¤ì¹˜ í•„ìš”: pip install konlpy)
try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ KoNLPyê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ ì œê±° ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜ ëª…ë ¹: pip install konlpy")
    KONLPY_AVAILABLE = False


# ============================================================================
# 5. ğŸ“‚ ê²½ë¡œ ê´€ë¦¬ (BASE_DIR ì„¤ì •)
# ============================================================================

# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ BASE_DIR ì„¤ì •
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))

def get_project_path(*paths) -> str:
    """
    í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ìƒì„±
    
    Args:
        *paths: ê²½ë¡œ ì¡°ê°ë“¤
        
    Returns:
        ì ˆëŒ€ ê²½ë¡œ ë¬¸ìì—´
        
    Example:
        get_project_path('data', 'chroma_db')
        -> 'C:/LDG_CODES/SKN20/SKN20-3rd-3TEAM/data/chroma_db'
    """
    return os.path.join(BASE_DIR, *paths)


# ============================================================================
# 4. ğŸ“ ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™” ì„¤ì •
# ============================================================================

# ìµœì í™”ëœ ì²­í¬ ì„¤ì • (ìˆ˜ì˜í•™ ì„ìƒ ë¬¸ë§¥ ìœ ì§€)
CHUNK_SIZE = 512  # í† í° ê¸°ì¤€
CHUNK_OVERLAP = 80  # í† í° ê¸°ì¤€

# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ë¥¼ ìœ„í•œ êµ¬ë¶„ì ìš°ì„ ìˆœìœ„
KOREAN_SEPARATORS = [
    "\n\n",  # ë‹¨ë½ êµ¬ë¶„
    "\n",    # ì¤„ë°”ê¿ˆ
    ". ",    # ë¬¸ì¥ ì¢…ë£Œ
    "? ",    # ì˜ë¬¸ë¬¸
    "! ",    # ê°íƒ„ë¬¸
    "; ",    # ì„¸ë¯¸ì½œë¡ 
    ", ",    # ì‰¼í‘œ
    " ",     # ê³µë°±
    ""       # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
]


def create_optimized_text_splitter() -> RecursiveCharacterTextSplitter:
    """
    ìµœì í™”ëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±
    
    Returns:
        RecursiveCharacterTextSplitter ì¸ìŠ¤í„´ìŠ¤
    """
    return RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=KOREAN_SEPARATORS,
        is_separator_regex=False
    )


# ============================================================================
# 1. ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ (Query Re-writing)
# ============================================================================

def extract_keywords_for_query(
    user_input: str,
    llm_model: Optional[ChatOpenAI] = None
) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ RAG ê²€ìƒ‰ì— ìµœì í™”ëœ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    Args:
        user_input: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
        llm_model: ì‚¬ìš©í•  LLM ëª¨ë¸ (Noneì´ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
        
    Returns:
        ë„ì–´ì“°ê¸°ë¡œ ì—°ê²°ëœ í‚¤ì›Œë“œ ë¬¸ìì—´
        
    Example:
        Input: "ì €í¬ ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ ê³„ì†í•˜ê³  í™©ë‹¬ ì¦ìƒì´ ìˆì–´ìš”"
        Output: "êµ¬í†  í™©ë‹¬ ê°„ì§ˆí™˜ ë‚´ê³¼ ì„±ê²¬"
    """
    if llm_model is None:
        llm_model = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.0,  # ì¼ê´€ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
            max_tokens=150
        )
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
    keyword_prompt = f"""ë‹¤ìŒ ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§ˆë¬¸ì—ì„œ RAG ê²€ìƒ‰ì— í•„ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{user_input}

## ì¶”ì¶œ ê¸°ì¤€ (4ê°€ì§€ ë²”ì£¼)
1. **ì§ˆë³‘ëª…**: ì˜ì‹¬ë˜ëŠ” ì§ˆí™˜ (ì˜ˆ: ìœ„ì—¼, ê°„ì§ˆí™˜, ì™¸ì´ë„ì—¼)
2. **ì¦ìƒ**: ëª…í™•í•œ ì¦ìƒ í‚¤ì›Œë“œ (ì˜ˆ: êµ¬í† , ì„¤ì‚¬, í™©ë‹¬, ê¸°ì¹¨)
3. **ì‹ ì²´ ë¶€ìœ„**: ì˜í–¥ë°›ëŠ” ë¶€ìœ„ (ì˜ˆ: ëˆˆ, ê·€, í”¼ë¶€, ë³µë¶€)
4. **ì—°ë ¹ëŒ€**: ìê²¬/ì„±ê²¬/ë…¸ë ¹ê²¬ (ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ ìƒëµ)

## ì¶œë ¥ í˜•ì‹
**ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë§Œ** ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ ì—†ì´ í‚¤ì›Œë“œë§Œ ë‚˜ì—´í•˜ì„¸ìš”.

ì˜ˆì‹œ:
- ì…ë ¥: "3ê°œì›” ëœ ê°•ì•„ì§€ê°€ ì„¤ì‚¬í•˜ê³  í† í•´ìš”"
- ì¶œë ¥: "ì„¤ì‚¬ êµ¬í†  ì¥ì—¼ ìê²¬ ë‚´ê³¼"

í‚¤ì›Œë“œ:"""

    try:
        response = llm_model.invoke([HumanMessage(content=keyword_prompt)])
        keywords = response.content.strip()
        
        # ì¶”ê°€ ì •ì œ: ë¶ˆí•„ìš”í•œ êµ¬ë‘ì  ì œê±°
        keywords = re.sub(r'[^\w\s]', '', keywords)
        keywords = re.sub(r'\s+', ' ', keywords)
        
        print(f"[í‚¤ì›Œë“œ ì¶”ì¶œ] ì›ë³¸: {user_input[:50]}...")
        print(f"[í‚¤ì›Œë“œ ì¶”ì¶œ] ê²°ê³¼: {keywords}")
        
        return keywords
    
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return user_input


# ============================================================================
# 2. ğŸ—‘ï¸ ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)
# ============================================================================

# í•œêµ­ì–´ ì˜í•™ ë„ë©”ì¸ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
KOREAN_MEDICAL_STOPWORDS = {
    # ì¼ë°˜ ë¶ˆìš©ì–´
    'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ë˜', 'ë•Œë¬¸',
    'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•´', 'ë”°ë¼', 'ì˜í•´', 'ë¡œì¨', 'ë¶€í„°',
    'ê¹Œì§€', 'ë§ˆë‹¤', 'ì¡°ì°¨', 'ë§Œ', 'ë¿', 'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ',
    
    # ì˜í•™ ë¬¸ì„œ ë¶ˆìš©ì–´
    'ì¦ë¡€', 'í™˜ì', 'ë³´í˜¸ì', 'ìˆ˜ì˜ì‚¬', 'ë³‘ì›', 'ì§„ë£Œ', 'ê²€ì‚¬',
    'ê²°ê³¼', 'ì†Œê²¬', 'íŒë‹¨', 'í™•ì¸', 'ê´€ì°°', 'í•„ìš”', 'ê°€ëŠ¥',
    
    # ì¡°ì‚¬/ì–´ë¯¸
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
    'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'í•˜ê³ ', 'í•˜ë©°', 'ë˜ì–´',
}


def preprocess_text_with_stopwords(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆìš©ì–´ ì œê±° (í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        ë¶ˆìš©ì–´ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
        
    Example:
        Input: "ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤"
        Output: "ê°•ì•„ì§€ êµ¬í† "
    """
    if not text or not text.strip():
        return ""
    
    # KoNLPy ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    if not KONLPY_AVAILABLE:
        # KoNLPy ì—†ì„ ê²½ìš° ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ì œê±°ë§Œ ìˆ˜í–‰
        return _simple_stopword_removal(text)
    
    try:
        okt = Okt()
        
        # í˜•íƒœì†Œ ë¶„ì„
        morphs = okt.pos(text, norm=True, stem=True)
        
        # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ (ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì  ì œê±°)
        filtered_words = []
        for word, pos in morphs:
            # ìœ ì˜ë¯¸í•œ í’ˆì‚¬ë§Œ ì„ íƒ
            if pos in ['Noun', 'Verb', 'Adjective']:
                # ë¶ˆìš©ì–´ ì œì™¸
                if word not in KOREAN_MEDICAL_STOPWORDS and len(word) > 1:
                    filtered_words.append(word)
        
        # ë„ì–´ì“°ê¸°ë¡œ ì—°ê²°
        cleaned_text = ' '.join(filtered_words)
        
        return cleaned_text
    
    except Exception as e:
        print(f"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}, ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ì œê±°ë¡œ ëŒ€ì²´")
        return _simple_stopword_removal(text)


def _simple_stopword_removal(text: str) -> str:
    """
    ê°„ë‹¨í•œ ë¶ˆìš©ì–´ ì œê±° (KoNLPy ì—†ì„ ë•Œ ëŒ€ì²´ ë°©ë²•)
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        ë¶ˆìš©ì–´ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    # êµ¬ë‘ì  ì œê±°
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # ë‹¨ì–´ ë¶„ë¦¬
    words = text.split()
    
    # ë¶ˆìš©ì–´ ì œê±°
    filtered_words = [
        word for word in words 
        if word not in KOREAN_MEDICAL_STOPWORDS and len(word) > 1
    ]
    
    return ' '.join(filtered_words)


# ============================================================================
# 3. ğŸ’¾ ëª¨ë¸ ë° ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥/ë¡œë“œ
# ============================================================================

def save_processed_documents(
    documents: List[Document],
    save_path: str
) -> None:
    """
    ì „ì²˜ë¦¬ëœ Document ê°ì²´ë“¤ì„ pickleë¡œ ì €ì¥
    
    Args:
        documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        save_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    with open(save_path, 'wb') as f:
        pickle.dump(documents, f)
    
    print(f"âœ“ ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}")
    print(f"  - ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")


def load_processed_documents(load_path: str) -> Optional[List[Document]]:
    """
    ì €ì¥ëœ Document ê°ì²´ë“¤ì„ pickleì—ì„œ ë¡œë“œ
    
    Args:
        load_path: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        
    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (íŒŒì¼ ì—†ì„ ì‹œ)
    """
    if not os.path.exists(load_path):
        print(f"â„¹ï¸ ì €ì¥ëœ ì „ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ: {load_path}")
        return None
    
    try:
        with open(load_path, 'rb') as f:
            documents = pickle.load(f)
        
        print(f"âœ“ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {load_path}")
        print(f"  - ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
        
        return documents
    
    except Exception as e:
        print(f"âš ï¸ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def manage_persistence(
    data_path: str,
    persist_dir: str,
    force_rebuild: bool = False
) -> Dict[str, Any]:
    """
    Vector DB ë° ì „ì²˜ë¦¬ ê²°ê³¼ ì˜êµ¬ ì €ì¥/ë¡œë“œ ê´€ë¦¬
    
    ì „ì²´ íë¦„:
    1. Vector DB ì¡´ì¬ â†’ ë¡œë“œ
    2. Vector DB ì—†ìŒ + pkl ì¡´ì¬ â†’ pkl ë¡œë“œ â†’ ì„ë² ë”© â†’ Vector DB ì €ì¥
    3. ëª¨ë‘ ì—†ìŒ â†’ ì›ì²œ ë°ì´í„° ë¡œë“œ â†’ ì „ì²˜ë¦¬ â†’ pkl ì €ì¥ â†’ ì„ë² ë”© â†’ Vector DB ì €ì¥
    
    Args:
        data_path: ì›ì²œ ë°ì´í„° ê²½ë¡œ
        persist_dir: Vector DB ì €ì¥ ë””ë ‰í† ë¦¬
        force_rebuild: Trueë©´ ìºì‹œ ë¬´ì‹œí•˜ê³  ì¬êµ¬ì¶•
        
    Returns:
        {
            "documents": List[Document],
            "vectorstore": Chroma,
            "retriever": Retriever,
            "status": "loaded" | "created"
        }
    """
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    
    pkl_path = get_project_path('data', 'processed_docs.pkl')
    
    # ê°•ì œ ì¬êµ¬ì¶• í”Œë˜ê·¸
    if force_rebuild:
        print("ğŸ”„ ê°•ì œ ì¬êµ¬ì¶• ëª¨ë“œ: ê¸°ì¡´ ìºì‹œ ë¬´ì‹œ")
        if os.path.exists(persist_dir):
            import shutil
            shutil.rmtree(persist_dir)
        if os.path.exists(pkl_path):
            os.remove(pkl_path)
    
    # ========================================================================
    # 1ë‹¨ê³„: Vector DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    # ========================================================================
    if os.path.exists(persist_dir) and os.path.exists(os.path.join(persist_dir, 'chroma.sqlite3')):
        print(f"âœ“ ê¸°ì¡´ Vector DB ë°œê²¬: {persist_dir}")
        print("  - Vector DB ë¡œë“œ ì¤‘...")
        
        try:
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = Chroma(
                persist_directory=persist_dir,
                embedding_function=embeddings
            )
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            )
            
            print("âœ“ Vector DB ë¡œë“œ ì™„ë£Œ")
            
            return {
                "documents": None,  # ì´ë¯¸ ì„ë² ë”©ë¨
                "vectorstore": vectorstore,
                "retriever": retriever,
                "status": "loaded"
            }
        
        except Exception as e:
            print(f"âš ï¸ Vector DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("  - ì¬êµ¬ì¶• ì§„í–‰...")
    
    # ========================================================================
    # 2ë‹¨ê³„: pkl íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    # ========================================================================
    documents = load_processed_documents(pkl_path)
    
    if documents is None:
        print(f"â„¹ï¸ ì „ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ, ì›ì²œ ë°ì´í„° ë¡œë“œ ì‹œì‘: {data_path}")
        
        # ì›ì²œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        from data.preprocessing import load_multiple_departments
        
        documents = load_multiple_departments(
            base_path=data_path,
            departments=["ë‚´ê³¼", "ì™¸ê³¼", "ì•ˆê³¼", "ì¹˜ê³¼", "í”¼ë¶€ê³¼"],
            data_type="source",
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        
        if not documents:
            raise ValueError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {data_path}")
        
        # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
        save_processed_documents(documents, pkl_path)
    
    # ========================================================================
    # 3ë‹¨ê³„: Vector DB êµ¬ì¶•
    # ========================================================================
    print(f"\nğŸ“Š Vector DB êµ¬ì¶• ì‹œì‘ (ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ)")
    
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_dir
    )
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}
    )
    
    print(f"âœ“ Vector DB êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ: {persist_dir}")
    
    return {
        "documents": documents,
        "vectorstore": vectorstore,
        "retriever": retriever,
        "status": "created"
    }


# ============================================================================
# í†µí•© ìµœì í™” ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def optimized_preprocess_text(text: str) -> str:
    """
    ìµœì í™”ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë¶ˆìš©ì–´ ì œê±° + ì •ì œ)
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        
    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    
    # 1. íŠ¹ìˆ˜ë¬¸ì ì •ì œ (ì˜í•™ ìš©ì–´ ë³´ì¡´)
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # 2. ì—°ì† ê³µë°± í†µí•©
    text = re.sub(r'\s+', ' ', text)
    
    # 3. ë¶ˆìš©ì–´ ì œê±°
    text = preprocess_text_with_stopwords(text)
    
    # 4. ìµœì¢… ì •ì œ
    text = text.strip()
    
    return text


# ============================================================================
# ì˜ˆì œ ì‚¬ìš©ë²•
# ============================================================================

if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    
    print("="*80)
    print("RAG ì‹œìŠ¤í…œ ìµœì í™” ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # 1. ê²½ë¡œ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n[1] ê²½ë¡œ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
    print(f"BASE_DIR: {BASE_DIR}")
    print(f"Data Path: {get_project_path('data', 'chroma_db')}")
    
    # 2. í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    print("\n[2] í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸")
    test_query = "ì €í¬ ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ ê³„ì†í•˜ê³  í™©ë‹¬ ì¦ìƒì´ ìˆì–´ìš”. 3ì‚´ ëœ ì„±ê²¬ì…ë‹ˆë‹¤."
    keywords = extract_keywords_for_query(test_query)
    print(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
    
    # 3. ë¶ˆìš©ì–´ ì œê±° í…ŒìŠ¤íŠ¸
    print("\n[3] ë¶ˆìš©ì–´ ì œê±° í…ŒìŠ¤íŠ¸")
    test_text = "ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì‹¬ê°í•œ ì¦ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    cleaned = preprocess_text_with_stopwords(test_text)
    print(f"ì›ë³¸: {test_text}")
    print(f"ì •ì œ: {cleaned}")
    
    # 4. ì²­í¬ ë¶„í•  í…ŒìŠ¤íŠ¸
    print("\n[4] ì²­í¬ ë¶„í•  í…ŒìŠ¤íŠ¸")
    splitter = create_optimized_text_splitter()
    print(f"ì²­í¬ í¬ê¸°: {CHUNK_SIZE}, ì˜¤ë²„ë©: {CHUNK_OVERLAP}")
    
    # 5. ì˜êµ¬ ì €ì¥ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n[5] ì˜êµ¬ ì €ì¥ ê´€ë¦¬ í…ŒìŠ¤íŠ¸")
    print("manage_persistence() í•¨ìˆ˜ëŠ” ì‹¤ì œ ë°ì´í„°ê°€ ìˆì„ ë•Œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    print("\nâœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
