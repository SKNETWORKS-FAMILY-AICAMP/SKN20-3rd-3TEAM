"""
ë°ì´í„° ì „ì²˜ë¦¬ ë° ì²­í‚¹ ëª¨ë“ˆ
ê¸°ì¡´ data preprocessing.pyë¥¼ í´ë˜ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë¦¬íŒ©í† ë§
"""

import os
import json
import glob
import warnings
from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

warnings.filterwarnings("ignore")


class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì²­í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self, base_data_path: str = None, project_root: str = None):
        """
        Args:
            base_data_path: ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ (Noneì´ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
            project_root: í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        """
        if base_data_path is None:
            # project_rootê°€ ì£¼ì–´ì§„ ê²½ìš° ì‚¬ìš©, ì•„ë‹ˆë©´ ìë™ ê°ì§€
            if project_root is not None:
                current_dir = project_root
            else:
                current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            self.base_path = os.path.join(current_dir, "1.ë°ì´í„°", "Training", "02.ë¼ë²¨ë§ë°ì´í„°")
        else:
            self.base_path = base_data_path
            
        # ì²­í‚¹ ì „ëµ ì„¤ì •
        self.splitters = {
            "medical_data": RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=100,
                separators=['\n\n', '\n', '.', '!', '?', ',', ' ', '']
            ),
            "qa_data": RecursiveCharacterTextSplitter(
                chunk_size=800,
                chunk_overlap=50,
                separators=['\n\nA:', 'Q:', '\n\n', '\n', '.', ' ', '']
            )
        }
        
    def load_medical_data(self, paths: List[str]) -> List[Document]:
        """
        ì˜í•™ì§€ì‹ ë°ì´í„° ë¡œë“œ (ë§ë­‰ì¹˜)
        
        Args:
            paths: ë§ë­‰ì¹˜ ë°ì´í„° í´ë” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        docs = []
        
        for path in paths:
            if not os.path.exists(path):
                print(f"âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
                continue
                
            print(f"ì²˜ë¦¬ ì¤‘: {path}")
            
            for file_path in glob.glob(os.path.join(path, "**", "*.json"), recursive=True):
                try:
                    with open(file_path, "r", encoding="utf-8-sig") as f:
                        data = json.load(f)
                    
                    disease = data.get("disease", "") or ""
                    
                    meta = {
                        "title": data.get("title", ""),
                        "author": data.get("author", None),
                        "publisher": data.get("publisher", None),
                        "department": data.get("department", None),
                        "source_type": "medical_data",
                        "source_path": path,
                    }
                    
                    docs.append(Document(page_content=disease, metadata=meta))
                except Exception as e:
                    print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                    continue
        
        print(f"âœ“ ì´ {len(docs)}ê°œ ì˜í•™ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return docs
    
    def load_qa_data(self, paths: List[str]) -> List[Document]:
        """
        ì§ˆì˜ì‘ë‹µ ë°ì´í„° ë¡œë“œ
        
        Args:
            paths: QA ë°ì´í„° í´ë” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        docs_qa = []
        
        for path_qa in paths:
            if not os.path.exists(path_qa):
                print(f"âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path_qa}")
                continue
                
            print(f"ì²˜ë¦¬ ì¤‘: {path_qa}")
            
            for file_path in glob.glob(os.path.join(path_qa, "**", "*.json"), recursive=True):
                try:
                    with open(file_path, "r", encoding="utf-8-sig") as f:
                        data = json.load(f)
                    
                    meta_info = data.get("meta", {})
                    qa_info = data.get("qa", {})
                    
                    question = qa_info.get("input", "")
                    answer = qa_info.get("output", "")
                    
                    page_content = f"Q: {question}\n\nA: {answer}"
                    
                    metadata = {
                        "lifeCycle": meta_info.get("lifeCycle", ""),
                        "department": meta_info.get("department", ""),
                        "disease": meta_info.get("disease", ""),
                        "question": question,
                        "answer": answer,
                        "source_type": "qa_data",
                        "source_path": path_qa
                    }
                    
                    docs_qa.append(Document(page_content=page_content, metadata=metadata))
                except Exception as e:
                    print(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({file_path}): {e}")
                    continue
        
        print(f"âœ“ ì´ {len(docs_qa)}ê°œ QA ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return docs_qa
    
    def chunk_documents(self, docs: List[Document]) -> List[Document]:
        """
        ë¬¸ì„œ ì²­í‚¹
        
        Args:
            docs: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì²­í‚¹ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        chunked_docs = []
        
        print(f"\nì²­í‚¹ ì‹œì‘: {len(docs)}ê°œ ë¬¸ì„œ")
        
        for doc in docs:
            source_type = doc.metadata.get("source_type", "")
            
            if source_type == "medical_data":
                splitter = self.splitters["medical_data"]
            elif source_type == "qa_data":
                splitter = self.splitters["qa_data"]
            else:
                continue
            
            chunks = splitter.split_documents([doc])
            
            for i, chunk in enumerate(chunks):
                chunk.metadata.update({
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "chunk_method": source_type
                })
            
            chunked_docs.extend(chunks)
        
        print(f"âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunked_docs)}ê°œ ì²­í¬ ìƒì„±\n")
        return chunked_docs
    
    def process_all_data(self) -> List[Document]:
        """
        ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        
        Returns:
            ì²­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸
        """
        print("\n" + "="*60)
        print("ğŸš€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("="*60)
        
        # QA ë°ì´í„° ê²½ë¡œ
        qa_paths = [
            os.path.join(self.base_path, "TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_ë‚´ê³¼"),
            os.path.join(self.base_path, "TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_ì•ˆê³¼"),
            os.path.join(self.base_path, "TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_ì™¸ê³¼"),
            os.path.join(self.base_path, "TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_ì¹˜ê³¼"),
            os.path.join(self.base_path, "TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_í”¼ë¶€ê³¼"),
        ]
        
        # ë°ì´í„° ë¡œë“œ
        print("\nğŸ“„ QA ë°ì´í„° ë¡œë“œ...")
        docs = self.load_qa_data(qa_paths)
        
        # ì²­í‚¹
        print("\nâœ‚ï¸ ë¬¸ì„œ ì²­í‚¹...")
        chunked_docs = self.chunk_documents(docs)
        
        print("\n" + "="*60)
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(chunked_docs)}ê°œ ì²­í¬")
        print("="*60 + "\n")
        
        return chunked_docs


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    preprocessor = DataPreprocessor()
    docs = preprocessor.process_all_data()
    
    # ê²°ê³¼ ì €ì¥
    import pickle
    with open("chunked_docs.pkl", "wb") as f:
        pickle.dump(docs, f)
    print("âœ“ chunked_docs.pkl ì €ì¥ ì™„ë£Œ")
