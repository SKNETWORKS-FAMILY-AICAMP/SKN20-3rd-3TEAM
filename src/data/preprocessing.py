"""
ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ëª¨ë“ˆ
ë°˜ë ¤ë™ë¬¼ ì§ˆí™˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ Document ê°ì²´ë¡œ ë³€í™˜
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import re

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# ìµœì í™” ëª¨ë“ˆ import
try:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils.optimization import (
        preprocess_text_with_stopwords,
        CHUNK_SIZE,
        CHUNK_OVERLAP,
        KOREAN_SEPARATORS
    )
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    print("âš ï¸ ìµœì í™” ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    OPTIMIZATION_AVAILABLE = False
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    KOREAN_SEPARATORS = ["\n\n", "\n", ". ", " ", ""]


def clean_text(text: str, remove_stopwords: bool = False) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ì œ: ë¶ˆí•„ìš”í•œ ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        remove_stopwords: Trueë©´ ë¶ˆìš©ì–´ ì œê±° ìˆ˜í–‰
        
    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
    text = re.sub(r'\s+', ' ', text)
    
    # íŠ¹ìˆ˜ ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ë¶ˆìš©ì–´ ì œê±° (ì˜µì…˜)
    if remove_stopwords and OPTIMIZATION_AVAILABLE:
        text = preprocess_text_with_stopwords(text)
    
    return text


def _remove_boilerplate_text(text: str) -> str:
    """
    ë‹µë³€ì—ì„œ ì •í˜•í™”ëœ ê´€ë¦¬ ì§€ì¹¨ ë¬¸êµ¬ ì œê±° (ì§€ì¹¨ 4.2: í›„ì²˜ë¦¬)
    
    Args:
        text: ì›ë³¸ ë‹µë³€ í…ìŠ¤íŠ¸
        
    Returns:
        ì •í˜•í™” ë¬¸êµ¬ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    # ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ì •í˜•í™” ë¬¸êµ¬ íŒ¨í„´
    boilerplate_patterns = [
        r'ë‚ ì§œë¥¼ í•¨ê»˜ ì ì–´ ë¹„êµí•´\s*ì£¼ì„¸ìš”[.ã€‚]*',
        r'ê¸°ë¡ì„ ë‚¨ê²¨\s*ì£¼ì„¸ìš”[.ã€‚]*',
        r'ìˆ˜ì˜ì‚¬ì™€\s*ìƒë‹´.*ê¶Œì¥.*',
        r'ë°˜ë ¤ë™ë¬¼.*ê±´ê°•.*ê¸°ì›.*'
    ]
    
    for pattern in boilerplate_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    return text.strip()


def load_json_file(file_path: str) -> Dict[str, Any]:
    """
    JSON íŒŒì¼ ë¡œë“œ
    
    Args:
        file_path: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        íŒŒì‹±ëœ JSON ë”•ì…”ë„ˆë¦¬
    """
    try:
        # UTF-8 BOM ì²˜ë¦¬ë¥¼ ìœ„í•´ utf-8-sig ì‚¬ìš©
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return {}


def load_and_preprocess_data(
    file_path: str,
    chunk_size: int = None,  # Noneì´ë©´ ìµœì í™”ëœ ê°’ ì‚¬ìš©
    chunk_overlap: int = None,  # Noneì´ë©´ ìµœì í™”ëœ ê°’ ì‚¬ìš©
    data_type: str = "source",  # "source" ë˜ëŠ” "labeled"
    remove_stopwords: bool = False  # ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€
) -> List[Document]:
    """
    ì›ì²œ ë°ì´í„°(ë˜ëŠ” ë¼ë²¨ë§ ë°ì´í„°)ë¥¼ ë¡œë“œí•˜ê³  LangChain Document ê°ì²´ë¡œ ë³€í™˜
    
    Args:
        file_path: JSON íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        chunk_size: ì²­í¬ í¬ê¸° (í† í° ìˆ˜ ê¸°ì¤€)
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë© í¬ê¸°
        data_type: "source" (ì›ì²œ ë°ì´í„°) ë˜ëŠ” "labeled" (ë¼ë²¨ë§ ë°ì´í„°)
        
    Returns:
        LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    documents = []
    
    # ìµœì í™”ëœ ì²­í¬ ì„¤ì • ì ìš©
    if chunk_size is None:
        chunk_size = CHUNK_SIZE
    if chunk_overlap is None:
        chunk_overlap = CHUNK_OVERLAP
    
    print(f"ğŸ“ ì²­í¬ ì„¤ì •: size={chunk_size}, overlap={chunk_overlap}")
    if remove_stopwords:
        print(f"ğŸ—‘ï¸ ë¶ˆìš©ì–´ ì œê±°: í™œì„±í™”")
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (ìµœì í™”ëœ êµ¬ë¶„ì ì‚¬ìš©)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=KOREAN_SEPARATORS if OPTIMIZATION_AVAILABLE else ["\n\n", "\n", ". ", " ", ""]
    )
    
    # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
    path = Path(file_path)
    
    if path.is_file():
        files_to_process = [path]
    elif path.is_dir():
        files_to_process = list(path.glob("*.json"))
    else:
        print(f"Invalid path: {file_path}")
        return documents
    
    # ê° JSON íŒŒì¼ ì²˜ë¦¬
    for file in files_to_process:
        data = load_json_file(str(file))
        
        if not data:
            continue
        
        if data_type == "source":
            # ì›ì²œ ë°ì´í„° ì²˜ë¦¬ (TS_ë§ë­‰ì¹˜ë°ì´í„°)
            documents.extend(_process_source_data(data, text_splitter, str(file), remove_stopwords))
        elif data_type == "labeled":
            # ë¼ë²¨ë§ ë°ì´í„° ì²˜ë¦¬ (TL_ì§ˆì˜ì‘ë‹µë°ì´í„°)
            documents.extend(_process_labeled_data(data, text_splitter, str(file), remove_stopwords))
    
    print(f"ì´ {len(documents)}ê°œì˜ Document ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return documents


def _process_source_data(
    data: Dict[str, Any],
    text_splitter: RecursiveCharacterTextSplitter,
    file_path: str,
    remove_stopwords: bool = False
) -> List[Document]:
    """
    ì›ì²œ ë°ì´í„°(TS_ë§ë­‰ì¹˜ë°ì´í„°) ì²˜ë¦¬
    
    êµ¬ì¡°:
    {
        "title": "ì†Œë™ë¬¼ ì£¼ìš” ì§ˆí™˜ì˜ ì„ìƒì¶”ë¡ ...",
        "author": "í˜„ì°½ë°± ë‚´ê³¼ì•„ì¹´ë°ë¯¸ ì—­",
        "publisher": "(ì£¼)ë²”ë¬¸ì—ë“€ì¼€ì´ì…˜",
        "department": "ë‚´ê³¼",
        "disease": "í™©ë‹¬ ì¦ë¡€ì˜ ì„ìƒì  ì¶”ë¡ ... (ê¸´ í…ìŠ¤íŠ¸)"
    }
    """
    documents = []
    
    # disease í•„ë“œ ì¶”ì¶œ ë° ì •ì œ
    disease_text = data.get("disease", "")
    if not disease_text:
        return documents
    
    disease_text = clean_text(disease_text, remove_stopwords=remove_stopwords)
    
    # ì‘ê¸‰ë„ í‚¤ì›Œë“œ ê°ì§€ (ì§€ì¹¨ 5: ì‘ê¸‰ë„ íŒë‹¨ ì •í™•ë„ ê°œì„ )
    urgency_level = _detect_urgency_from_text(disease_text)
    
    # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§€ì¹¨ 4.1: departmentë¥¼ metadataì— í¬í•¨)
    metadata = {
        "title": data.get("title", ""),
        "author": data.get("author", ""),
        "publisher": data.get("publisher", ""),
        "department": data.get("department", ""),  # í•„í„°ë§ìš©
        "source_file": file_path,
        "data_type": "source",
        "urgency": urgency_level  # ì‘ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜ ì‘ê¸‰ë„
    }
    
    # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
    chunks = text_splitter.split_text(disease_text)
    
    # ê° ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    for i, chunk in enumerate(chunks):
        chunk_metadata = metadata.copy()
        chunk_metadata["chunk_id"] = i
        chunk_metadata["total_chunks"] = len(chunks)
        
        doc = Document(
            page_content=chunk,
            metadata=chunk_metadata
        )
        documents.append(doc)
    
    return documents


def _detect_urgency_from_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì‘ê¸‰ í‚¤ì›Œë“œë¥¼ ê°ì§€í•˜ì—¬ ì‘ê¸‰ë„ ì¶”ì •
    
    Args:
        text: ì§ˆí™˜ ì„¤ëª… í…ìŠ¤íŠ¸
        
    Returns:
        'High', 'Medium', 'Low'
    """
    text_lower = text.lower()
    
    # ê³ ìœ„í—˜ í‚¤ì›Œë“œ
    high_urgency_keywords = [
        'ë°œì‘', 'ê²½ë ¨', 'ì˜ì‹', 'í˜¸í¡ê³¤ë€', 'ì²­ìƒ‰ì¦', 'ì‡¼í¬', 'ì¶œí˜ˆ', 
        'ì‘ê¸‰', 'ì¦‰ì‹œ', 'ê¸‰ì„±', 'ì¤‘ë…', 'ë¹„í‹€ë¦¼', 'íŒ½ë§Œ', 'íƒˆìˆ˜',
        'ì €í˜ˆë‹¹', 'ê³ ì—´', 'íŒŒì—´', 'íìƒ‰', 'ì§ˆì‹'
    ]
    
    # ì¤‘ìœ„í—˜ í‚¤ì›Œë“œ
    medium_urgency_keywords = [
        'êµ¬í† ', 'ì„¤ì‚¬', 'ê¸°ì¹¨', 'ê°ì—¼', 'ì—¼ì¦', 'í†µì¦', 'ë°œì—´',
        'ë¶€ì¢…', 'ê¶¤ì–‘', 'ì¶œí˜ˆ', 'ì™¸ìƒ', 'ê³¨ì ˆ'
    ]
    
    # í‚¤ì›Œë“œ ë§¤ì¹­
    for keyword in high_urgency_keywords:
        if keyword in text_lower or keyword in text:
            return 'High'
    
    for keyword in medium_urgency_keywords:
        if keyword in text_lower or keyword in text:
            return 'Medium'
    
    return 'Low'


def _process_labeled_data(
    data: Dict[str, Any],
    text_splitter: RecursiveCharacterTextSplitter,
    file_path: str,
    remove_stopwords: bool = False
) -> List[Document]:
    """
    ë¼ë²¨ë§ ë°ì´í„°(TL_ì§ˆì˜ì‘ë‹µë°ì´í„°) ì²˜ë¦¬
    
    êµ¬ì¡°:
    {
        "meta": {"lifeCycle": "ìê²¬", "department": "ë‚´ê³¼", "disease": "ê¸°íƒ€"},
        "qa": {
            "instruction": "ë„ˆëŠ” ë°˜ë ¤ê²¬ ê±´ê°• ì „ë¬¸ê°€ì•¼...",
            "input": "ì €í¬ ì§‘ ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ í•˜ê³ ...",
            "output": "ì§ˆë¬¸ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤..."
        }
    }
    """
    documents = []
    
    meta = data.get("meta", {})
    qa = data.get("qa", {})
    
    if not qa:
        return documents
    
    # ë‹µë³€ì—ì„œ ì •í˜•í™”ëœ ê´€ë¦¬ ì§€ì¹¨ ë¬¸êµ¬ ì œê±° (ì§€ì¹¨ 4.2: í›„ì²˜ë¦¬)
    output_text = clean_text(qa.get('output', ''), remove_stopwords=remove_stopwords)
    output_text = _remove_boilerplate_text(output_text)
    
    # QA ìŒì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    qa_text = f"""ì§ˆë¬¸: {clean_text(qa.get('input', ''), remove_stopwords=remove_stopwords)}

ë‹µë³€: {output_text}"""
    
    # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§€ì¹¨ 4.2: lifeCycle, instruction í¬í•¨)
    metadata = {
        "life_cycle": meta.get("lifeCycle", ""),  # ì§ˆë¬¸ ì†ì„± ë¶€ì—¬ìš©
        "department": meta.get("department", ""),  # í•„í„°ë§ìš©
        "disease": meta.get("disease", ""),
        "instruction": qa.get("instruction", ""),  # System Prompt ì°¸ê³ ìš©
        "source_file": file_path,
        "data_type": "labeled"
    }
    
    # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (QAëŠ” ë³´í†µ ì§§ìœ¼ë¯€ë¡œ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬ë  ê°€ëŠ¥ì„± ë†’ìŒ)
    chunks = text_splitter.split_text(qa_text)
    
    # ê° ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
    for i, chunk in enumerate(chunks):
        chunk_metadata = metadata.copy()
        chunk_metadata["chunk_id"] = i
        chunk_metadata["total_chunks"] = len(chunks)
        
        doc = Document(
            page_content=chunk,
            metadata=chunk_metadata
        )
        documents.append(doc)
    
    return documents


def load_multiple_departments(
    base_path: str,
    departments: List[str] = ["ë‚´ê³¼", "ì™¸ê³¼", "ì•ˆê³¼", "ì¹˜ê³¼", "í”¼ë¶€ê³¼"],
    data_type: str = "source",
    chunk_size: int = None,  # Noneì´ë©´ ìµœì í™”ëœ ê°’ ì‚¬ìš©
    chunk_overlap: int = None,  # Noneì´ë©´ ìµœì í™”ëœ ê°’ ì‚¬ìš©
    remove_stopwords: bool = False  # ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€
) -> List[Document]:
    """
    ì—¬ëŸ¬ ì§„ë£Œê³¼ ë°ì´í„°ë¥¼ ì¼ê´„ ë¡œë“œ
    
    Args:
        base_path: ë°ì´í„° ê¸°ë³¸ ê²½ë¡œ
        departments: ë¡œë“œí•  ì§„ë£Œê³¼ ë¦¬ìŠ¤íŠ¸
        data_type: "source" ë˜ëŠ” "labeled"
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        
    Returns:
        ëª¨ë“  ì§„ë£Œê³¼ì˜ Document ë¦¬ìŠ¤íŠ¸
    """
    all_documents = []
    
    for dept in departments:
        if data_type == "source":
            dept_path = os.path.join(base_path, f"TS_ë§ë­‰ì¹˜ë°ì´í„°_{dept}")
        else:
            dept_path = os.path.join(base_path, f"TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_{dept}")
        
        if os.path.exists(dept_path):
            print(f"\n{dept} ë°ì´í„° ë¡œë“œ ì¤‘...")
            docs = load_and_preprocess_data(
                dept_path,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                data_type=data_type,
                remove_stopwords=remove_stopwords
            )
            all_documents.extend(docs)
        else:
            print(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dept_path}")
    
    return all_documents


# ì˜ˆì œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    # ì›ì²œ ë°ì´í„° ë¡œë“œ ì˜ˆì œ
    source_data_path = r"c:\LDG_CODES\SKN20\3rd_prj\data\59.ë°˜ë ¤ê²¬ ì„±ì¥ ë° ì§ˆë³‘ ê´€ë ¨ ë§ë­‰ì¹˜ ë°ì´í„°\3.ê°œë°©ë°ì´í„°\1.ë°ì´í„°\Training\01.ì›ì²œë°ì´í„°\TS_ë§ë­‰ì¹˜ë°ì´í„°_ë‚´ê³¼"
    
    print("=== ì›ì²œ ë°ì´í„° ë¡œë“œ ===")
    source_docs = load_and_preprocess_data(
        source_data_path,
        chunk_size=1000,
        chunk_overlap=200,
        data_type="source"
    )
    
    if source_docs:
        print(f"\nì²« ë²ˆì§¸ Document ìƒ˜í”Œ:")
        print(f"ë‚´ìš©: {source_docs[0].page_content[:200]}...")
        print(f"ë©”íƒ€ë°ì´í„°: {source_docs[0].metadata}")
    
    # ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ì˜ˆì œ
    labeled_data_path = r"c:\LDG_CODES\SKN20\3rd_prj\data\59.ë°˜ë ¤ê²¬ ì„±ì¥ ë° ì§ˆë³‘ ê´€ë ¨ ë§ë­‰ì¹˜ ë°ì´í„°\3.ê°œë°©ë°ì´í„°\1.ë°ì´í„°\Training\02.ë¼ë²¨ë§ë°ì´í„°\TL_ì§ˆì˜ì‘ë‹µë°ì´í„°_ë‚´ê³¼"
    
    print("\n\n=== ë¼ë²¨ë§ ë°ì´í„° ë¡œë“œ ===")
    labeled_docs = load_and_preprocess_data(
        labeled_data_path,
        chunk_size=1000,
        chunk_overlap=200,
        data_type="labeled"
    )
    
    if labeled_docs:
        print(f"\nì²« ë²ˆì§¸ Document ìƒ˜í”Œ:")
        print(f"ë‚´ìš©: {labeled_docs[0].page_content[:200]}...")
        print(f"ë©”íƒ€ë°ì´í„°: {labeled_docs[0].metadata}")
