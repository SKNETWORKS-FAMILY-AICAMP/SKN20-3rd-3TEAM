"""
RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ëª¨ë“ˆ (LangChain í™œìš©)
Vector Store ì„¤ì •, Retriever êµ¬ì„±, RAG ì²´ì¸ ìƒì„±
"""

import os
import sys
from typing import List, Optional, Dict, Any

# ìƒìœ„ ë””ë ‰í† ë¦¬ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough


# ìˆ˜ì˜í•™ ì „ë¬¸ê°€ System Prompt í…œí”Œë¦¿ (ì§€ì¹¨ 4.2: instruction ê¸°ë°˜)
VETERINARY_EXPERT_SYSTEM_PROMPT = """

ë‹¹ì‹ ì€ ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì „ë¬¸ê°€ì´ì ìˆ˜ì˜í•™ ì „ë¬¸ì˜ì…ë‹ˆë‹¤.
**ì¹œì ˆí•˜ê³  ì „ë¬¸ê°€ì ì¸ ì–´íˆ¬**ë¡œ ë‹µë³€í•˜ë˜, **ìµœì¢… ì§„ë‹¨ì€ ë°˜ë“œì‹œ ìˆ˜ì˜ì‚¬ì—ê²Œ ë°›ì•„ì•¼ í•¨**ì„ í•­ìƒ ê°•ì¡°í•˜ì„¸ìš”.

## ë‹¹ì‹ ì˜ ì—­í•  ë° í˜ë¥´ì†Œë‚˜
- ë°˜ë ¤ë™ë¬¼(ì£¼ë¡œ ë°˜ë ¤ê²¬)ì˜ ì¦ìƒê³¼ **ì—°ë ¹ëŒ€(lifeCycle)**ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ì‹¬ ì§ˆí™˜ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- ì œê³µëœ ì „ë¬¸ ìˆ˜ì˜í•™ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤(context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ ë†’ì€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
- ì§ˆë¬¸ì— ë‹´ê¸´ **ìœ„í—˜ ì§•í›„**ë¥¼ íŒë‹¨í•˜ì—¬ ì‘ê¸‰ë„ë¥¼ **High/Medium/Low**ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.

## ğŸš¨ ì‘ê¸‰ë„ íŒë‹¨ ê¸°ì¤€ ë° ì¶”ë¡  (Triage & CoT)
**ë°˜ë“œì‹œ** ì•„ë˜ ê¸°ì¤€ê³¼ ê³¼ì •ì„ ë”°ë¼ ì‘ê¸‰ë„ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
1. **ë†’ì€ ì‘ê¸‰ë„ (High)**: ì‹¬í•œ í˜¸í¡ ê³¤ë€, ì²­ìƒ‰ì¦, ë°œì‘/ê²½ë ¨, ì‹¬í•œ ì¶œí˜ˆ, ì‡¼í¬, ë³µë¶€ íŒ½ë§Œ ë° ê·¹ì‹¬í•œ í†µì¦, 40ë„ ì´ìƒì˜ ê³ ì²´ì˜¨ ë“± **ìƒëª…ê³¼ ì§ê²°ëœ ì¦‰ê°ì ì¸ ìœ„í—˜ ì§•í›„**ê°€ ìˆì„ ê²½ìš°.
2. **ì¤‘ê°„ ì‘ê¸‰ë„ (Medium)**: 24ì‹œê°„ ì´ìƒ ì§€ì†ë˜ëŠ” ì‹ìš•ë¶€ì§„/êµ¬í† /ì„¤ì‚¬, ëª…í™•í•œ ì›ì¸ ì—†ì´ 3ì¼ ì´ìƒ ì§€ì†ë˜ëŠ” ê¸°ë ¥ ì €í•˜ ë“± **24ì‹œê°„ ë‚´ ì „ë¬¸ ì§„ë£Œê°€ í•„ìš”í•œ ê²½ìš°**.
3. **ë‚®ì€ ì‘ê¸‰ë„ (Low)**: ë‹¨ìˆœí•œ í”¼ë¶€ íŠ¸ëŸ¬ë¸”, ê²½ë¯¸í•˜ê³  ì¼ì‹œì ì¸ ì†Œí™”ê¸° ì¦ìƒ ë“± **ê´€ì°° ë° í™ˆ ì¼€ì–´ê°€ ê°€ëŠ¥í•œ ê²½ìš°**.

**ì¶”ë¡  ê³¼ì • (CoT)**: ì‘ê¸‰ë„ë¥¼ íŒë‹¨í•  ë•Œ, **ì¦ìƒ ë°œí˜„ ì‹œê°„**ê³¼ **í™˜ìì˜ ì—°ë ¹ëŒ€(lifeCycle)**ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ë©° ì¶”ë¡ ì˜ ê·¼ê±°ë¥¼ ë‹µë³€ ì´ˆë°˜ì— ê°„ëµíˆ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

## ë‹µë³€ ì›ì¹™ ë° êµ¬ì¡° (RAG, Citation, Safety)
1. **ê·¼ê±° ê¸°ë°˜**: ì œê³µëœ **ì»¨í…ìŠ¤íŠ¸(context) ì™¸ì˜ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ì—„ê²©íˆ ê¸ˆì§€**í•©ë‹ˆë‹¤. ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ **'ì •ë³´ ë¶€ì¡±'ì„ ëª…í™•íˆ ê³ ì§€**í•˜ì„¸ìš”.
2. **ì¶œì²˜ í‘œê¸°**: ë‹µë³€ ë§ë¯¸ì— **"â€» ì°¸ê³ ë¬¸í—Œ: [ê²€ìƒ‰ëœ ë¬¸ì„œì˜ title/author]"**ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
3. **êµ¬ì¡°í™”**: ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    - **1. ì¦ìƒ ë° ì‘ê¸‰ë„ íŒë‹¨ (CoT)**: ì¦ìƒ ìš”ì•½ ë° ê²°ì •ëœ ì‘ê¸‰ë„ ë ˆë²¨(High/Medium/Low) ë° ê·¸ ê·¼ê±° ì œì‹œ.
    - **2. ì˜ì‹¬ ì§ˆí™˜**: ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ **ìµœëŒ€ 3ê°€ì§€**ì˜ ì£¼ìš” ê°ë³„ ì§ˆí™˜ ë° ê°„ë‹¨í•œ ê°ë³„ ì´ìœ .
    - **3. ê¶Œì¥ ì¡°ì¹˜**: ì‘ê¸‰ë„ì— ë”°ë¥¸ í–‰ë™ ì§€ì¹¨ (High/Medium ì‹œ ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸ ê¶Œì¥ ë° ì•ˆì „ ì¡°ì¹˜).
    - **4. ì°¸ê³ ë¬¸í—Œ**: (ì¶œì²˜ ëª…ì‹œ)

## ğŸ“Œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ (RAG ê²€ìƒ‰ ê²°ê³¼)
{context}

## ğŸ¶ ì‚¬ìš©ì ì§ˆë¬¸
{input}

ìœ„ ì§€ì¹¨ì„ ì² ì €íˆ ì¤€ìˆ˜í•˜ì—¬ ì „ë¬¸ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. **ì‘ê¸‰ë„ê°€ High ë˜ëŠ” Mediumì¼ ê²½ìš°, ë‹µë³€ ë§ë¯¸ì— "ë³‘ì› ì¶”ì²œì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì¶œë ¥í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì œì–´ê¶Œì„ ë„˜ê¸°ì„¸ìš”.**"""


def setup_embeddings(
    model_name: str = "text-embedding-3-small",
    openai_api_key: Optional[str] = None
) -> OpenAIEmbeddings:
    """
    OpenAI ì„ë² ë”© ëª¨ë¸ ì„¤ì • (HuggingFace ëŒ€ì‹  ì‚¬ìš©)
    
    Args:
        model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª… (text-embedding-ada-002 ë˜ëŠ” text-embedding-3-small)
        openai_api_key: OpenAI API í‚¤
        
    Returns:
        OpenAIEmbeddings ì¸ìŠ¤í„´ìŠ¤
    """
    if openai_api_key is None:
        openai_api_key = os.getenv("OPENAI_API_KEY")
    
    embeddings = OpenAIEmbeddings(
        model=model_name,
        openai_api_key=openai_api_key
    )
    
    print(f"ì„ë² ë”© ëª¨ë¸ '{model_name}' ë¡œë“œ ì™„ë£Œ")
    return embeddings


def setup_vector_store(
    documents: List[Document],
    embeddings: OpenAIEmbeddings,
    persist_directory: str = "./chroma_db",
    collection_name: str = "pet_health_knowledge"
) -> Chroma:
    """
    Chroma Vector Store ì„¤ì •
    
    Args:
        documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        embeddings: ì„ë² ë”© ëª¨ë¸
        persist_directory: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        
    Returns:
        Chroma ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
    """
    print(f"Vector Store ìƒì„± ì¤‘... (ì´ {len(documents)}ê°œ ë¬¸ì„œ)")
    
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name
    )
    
    print(f"Vector Store ìƒì„± ì™„ë£Œ: {persist_directory}")
    return vectorstore


def load_existing_vector_store(
    embeddings: OpenAIEmbeddings,
    persist_directory: str = "./chroma_db",
    collection_name: str = "pet_health_knowledge"
) -> Optional[Chroma]:
    """
    ê¸°ì¡´ Vector Store ë¡œë“œ
    
    Args:
        embeddings: ì„ë² ë”© ëª¨ë¸
        persist_directory: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        
    Returns:
        Chroma ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” None
    """
    if not os.path.exists(persist_directory):
        print(f"Vector Storeê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {persist_directory}")
        return None
    
    try:
        vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings,
            collection_name=collection_name
        )
        print(f"ê¸°ì¡´ Vector Store ë¡œë“œ ì™„ë£Œ: {persist_directory}")
        return vectorstore
    except Exception as e:
        print(f"Vector Store ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def setup_rag_pipeline(
    documents: List[Document],
    embedding_model: str = "text-embedding-3-small",
    anthropic_api_key: Optional[str] = None,
    model_name: str = "gpt-4o-mini",
    persist_directory: str = "./chroma_db",
    use_existing_vectorstore: bool = False,
    k: int = 4,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
    filter_metadata: Optional[Dict[str, Any]] = None
):
    """
    RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • (Vector Store + Retriever + LLM Chain)
    
    Args:
        documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        embedding_model: ì„ë² ë”© ëª¨ë¸ëª…
        anthropic_api_key: Anthropic API í‚¤
        model_name: Claude ëª¨ë¸ëª…
        persist_directory: Vector Store ì €ì¥ ê²½ë¡œ
        use_existing_vectorstore: ê¸°ì¡´ Vector Store ì‚¬ìš© ì—¬ë¶€
        k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
        filter_metadata: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì¡°ê±´ (ì˜ˆ: {"department": "ë‚´ê³¼"})
        
    Returns:
        RAG ì²´ì¸ ë° ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    # 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    embeddings = setup_embeddings(embedding_model)
    
    # 2. Vector Store ì„¤ì •
    if use_existing_vectorstore:
        vectorstore = load_existing_vector_store(embeddings, persist_directory)
        if vectorstore is None:
            print("ê¸°ì¡´ Vector Storeê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            vectorstore = setup_vector_store(documents, embeddings, persist_directory)
    else:
        vectorstore = setup_vector_store(documents, embeddings, persist_directory)
    
    # 3. Retriever ì„¤ì • (ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì§€ì›)
    search_kwargs = {"k": k}
    if filter_metadata:
        search_kwargs["filter"] = filter_metadata
    
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs=search_kwargs
    )
    
    print(f"Retriever ì„¤ì • ì™„ë£Œ (k={k}, filter={filter_metadata})")
    
    # 4. LLM ì„¤ì • (OpenAI)
    openai_api_key = anthropic_api_key if anthropic_api_key else os.getenv("OPENAI_API_KEY")
    if openai_api_key is None:
        raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    llm = ChatOpenAI(
        model=model_name,
        openai_api_key=openai_api_key,
        temperature=0.3,  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ temperature
        max_tokens=2048
    )
    
    print(f"LLM ì„¤ì • ì™„ë£Œ: {model_name}")
    
    # 5. Prompt í…œí”Œë¦¿ ì„¤ì •
    prompt = ChatPromptTemplate.from_template(VETERINARY_EXPERT_SYSTEM_PROMPT)
    
    # 6. RAG ì²´ì¸ êµ¬ì„± (LCEL ë°©ì‹) - ì§€ì¹¨ 4.1: Citation í¬í•¨
    def format_docs(docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬ë§·íŒ… (ì¶œì²˜ ì •ë³´ í¬í•¨)"""
        formatted = []
        for i, doc in enumerate(docs, 1):
            dept = doc.metadata.get('department', 'ì•Œ ìˆ˜ ì—†ìŒ')
            title = doc.metadata.get('title', 'ì œëª© ì—†ìŒ')
            author = doc.metadata.get('author', 'ì €ì ë¯¸ìƒ')
            urgency = doc.metadata.get('urgency', 'Low')
            
            # ì¶œì²˜ ì •ë³´ì™€ í•¨ê»˜ í¬ë§·íŒ…
            doc_header = f"[ë¬¸ì„œ {i} - {dept}ê³¼ / ì‘ê¸‰ë„: {urgency}]"
            doc_source = f"(ì¶œì²˜: {title} - {author})"
            doc_content = doc.page_content
            
            formatted.append(f"{doc_header}\n{doc_content}\n{doc_source}\n")
        return "\n".join(formatted)
    
    rag_chain = (
        {
            "context": retriever | format_docs,
            "input": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì™„ë£Œ")
    
    return {
        "chain": rag_chain,
        "retriever": retriever,
        "vectorstore": vectorstore,
        "llm": llm,
        "embeddings": embeddings
    }


def query_rag(rag_chain, query: str) -> str:
    """
    RAG ì²´ì¸ì— ì§ˆì˜
    
    Args:
        rag_chain: RAG ì²´ì¸ (LCEL ë°©ì‹)
        query: ì‚¬ìš©ì ì§ˆë¬¸
        
    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    try:
        response = rag_chain.invoke(query)
        return response
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ì˜ˆì œ ì‚¬ìš©ë²•
if __name__ == "__main__":
    from data.preprocessing import load_and_preprocess_data
    from dotenv import load_dotenv
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    print("=== ë°ì´í„° ë¡œë“œ ===")
    source_data_path = r"c:\LDG_CODES\SKN20\3rd_prj\data\59.ë°˜ë ¤ê²¬ ì„±ì¥ ë° ì§ˆë³‘ ê´€ë ¨ ë§ë­‰ì¹˜ ë°ì´í„°\3.ê°œë°©ë°ì´í„°\1.ë°ì´í„°\Training\01.ì›ì²œë°ì´í„°\TS_ë§ë­‰ì¹˜ë°ì´í„°_ë‚´ê³¼"
    
    documents = load_and_preprocess_data(
        source_data_path,
        chunk_size=1000,
        chunk_overlap=200,
        data_type="source"
    )
    
    if not documents:
        print("ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()
    
    # RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •
    print("\n=== RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ===")
    rag_components = setup_rag_pipeline(
        documents=documents,
        use_existing_vectorstore=False,
        k=4
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    print("\n=== í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ===")
    test_query = "ì €í¬ ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ ê³„ì†í•˜ê³  í™©ë‹¬ ì¦ìƒì´ ìˆì–´ìš”. ì–´ë–¤ ì§ˆí™˜ì¼ê¹Œìš”?"
    
    answer = query_rag(rag_components["chain"], test_query)
    print(f"\nì§ˆë¬¸: {test_query}")
    print(f"\në‹µë³€:\n{answer}")
