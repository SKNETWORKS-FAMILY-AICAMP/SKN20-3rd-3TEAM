"""
Evaluation Controller Module
ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë° íë¦„ ì œì–´

ì—­í• :
  - ë‹¤ì°¨ì› ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ì •í™•ë„, ëª…í™•ì„±, ì™„ì „ì„±, ì•ˆì „ì„±)
  - í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
  - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¡œê¹…
  - ì‘ë‹µ ê°œì„  í”¼ë“œë°± ìƒì„±
"""

from typing import Dict, Literal, Tuple


def evaluate_response(response: str) -> Dict[str, any]:
    """
    ì‘ë‹µì„ 4ê°œ ì°¨ì›ìœ¼ë¡œ í‰ê°€í•˜ê³  ì¢…í•© í‰ê°€ ê²°ê³¼ ë°˜í™˜
    
    Args:
        response (str): í‰ê°€í•  ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        Dict[str, any]: í‰ê°€ ê²°ê³¼
            {
                'pass': True/False,  # í‰ê°€ í†µê³¼ ì—¬ë¶€
                'scores': {
                    'accuracy': 0.0-1.0,      # ì •í™•ë„
                    'clarity': 0.0-1.0,        # ëª…í™•ì„±
                    'completeness': 0.0-1.0,  # ì™„ì „ì„±
                    'safety': 0.0-1.0          # ì•ˆì „ì„±
                },
                'average_score': 0.0-1.0,
                'feedback': 'ê°œì„  í”¼ë“œë°±',
                'reason': 'í‰ê°€ ê·¼ê±°'
            }
    
    í‰ê°€ ê¸°ì¤€:
        ğŸ“Š ì •í™•ë„ (Accuracy): 0.0-1.0
           - ì •ë³´ì˜ ì •í™•ì„±
           - ì‚¬ì‹¤ ê¸°ë°˜ ê²€ì¦
           
        ğŸ“Š ëª…í™•ì„± (Clarity): 0.0-1.0
           - ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
           - êµ¬ì¡°ì™€ í‘œí˜„ì´ ëª…í™•í•œê°€?
           
        ğŸ“Š ì™„ì „ì„± (Completeness): 0.0-1.0
           - ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí–ˆëŠ”ê°€?
           - í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ê°€?
           
        ğŸ“Š ì•ˆì „ì„± (Safety): 0.0-1.0
           - ì˜ë£Œ ì¡°ì–¸ì´ ì•ˆì „í•œê°€?
           - ë©´ì±… ì¡°í•­ì´ ìˆëŠ”ê°€?
           - ì‘ê¸‰ ìƒí™© í‘œí˜„ì´ ì ì ˆí•œê°€?
    
    í‰ê°€ íŒì •:
        âœ… pass = True (ì ìˆ˜ >= 0.75)
        ğŸ”„ pass = False (ì ìˆ˜ < 0.75)
    
    ì˜ˆì‹œ:
        ì…ë ¥: "ê°•ì•„ì§€ í”¼ë¶€ì—¼ì€ í”¼ë¶€ì˜ ì—¼ì¦ì…ë‹ˆë‹¤..."
        
        ì¶œë ¥:
        {
            'pass': True,
            'scores': {
                'accuracy': 0.90,
                'clarity': 0.85,
                'completeness': 0.80,
                'safety': 0.85
            },
            'average_score': 0.85,
            'feedback': 'ë‹µë³€ì´ ì •í™•í•˜ê³  ì™„ì „í•©ë‹ˆë‹¤',
            'reason': 'ëª¨ë“  í‰ê°€ í•­ëª©ì´ ìš°ìˆ˜í•¨'
        }
    
    TODO:
        - ê° ì°¨ì›ë³„ í‰ê°€ ë¡œì§
        - LLM ê¸°ë°˜ í‰ê°€ ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹±
        - ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜
    """
    # TODO: ë‹¤ì°¨ì› í‰ê°€ ë¡œì§
    
    print(f"âš–ï¸  [evaluate_response] ì‘ë‹µ í‰ê°€ ì¤‘...\n")
    
    # í‰ê°€ ìˆ˜í–‰
    scores = {
        'accuracy': check_accuracy(response),
        'clarity': check_clarity(response),
        'completeness': check_completeness(response),
        'safety': check_safety_guidelines(response)['passed']
    }
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    average_score = sum(scores.values()) / len(scores)
    
    # í‰ê°€ íŒì •
    passed = average_score >= 0.75
    
    # í”¼ë“œë°± ìƒì„±
    feedback = generate_feedback(scores, response)
    
    evaluation = {
        'pass': passed,
        'scores': scores,
        'average_score': average_score,
        'feedback': feedback,
        'reason': 'í‰ê°€ ì™„ë£Œ'
    }
    
    print(f"âœ“ í‰ê°€ ì™„ë£Œ")
    print(f"  - ì •í™•ë„: {scores['accuracy']:.0%}")
    print(f"  - ëª…í™•ì„±: {scores['clarity']:.0%}")
    print(f"  - ì™„ì „ì„±: {scores['completeness']:.0%}")
    print(f"  - ì•ˆì „ì„±: {scores['safety']:.0%}")
    print(f"  - í‰ê· : {average_score:.0%}")
    print(f"  - íŒì •: {'âœ… í†µê³¼' if passed else 'ğŸ”„ ì¬ì‘ì„± í•„ìš”'}\n")
    
    return evaluation


def check_accuracy(response: str) -> float:
    """
    ì‘ë‹µì˜ ì •í™•ë„ í‰ê°€
    
    Args:
        response (str): í‰ê°€í•  ì‘ë‹µ
        
    Returns:
        float: ì •í™•ë„ ì ìˆ˜ (0.0-1.0)
    
    í‰ê°€ ê¸°ì¤€:
        - ì˜ë£Œ ì •ë³´ì˜ ì •í™•ì„±
        - íŒ©íŠ¸ ì²´í¬
        - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜
    
    TODO:
        - íŒ©íŠ¸ ì²´í¬ ë¡œì§
        - ì˜ë£Œ ì •ë³´ ê²€ì¦
    """
    # TODO: ì •í™•ë„ í‰ê°€ ë¡œì§
    
    # ë”ë¯¸ ì ìˆ˜
    if len(response) > 100:
        accuracy = 0.85
    else:
        accuracy = 0.65
    
    return accuracy


def check_clarity(response: str) -> float:
    """
    ì‘ë‹µì˜ ëª…í™•ì„± í‰ê°€
    
    Args:
        response (str): í‰ê°€í•  ì‘ë‹µ
        
    Returns:
        float: ëª…í™•ì„± ì ìˆ˜ (0.0-1.0)
    
    í‰ê°€ ê¸°ì¤€:
        - ë¬¸ì¥ êµ¬ì¡°ì˜ ëª…í™•ì„±
        - ìš©ì–´ ì •ì˜
        - ê°€ë…ì„±
    
    TODO:
        - ê°€ë…ì„± ì§€í‘œ ê³„ì‚° (Flesch Reading Ease)
        - ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
    """
    # TODO: ëª…í™•ì„± í‰ê°€ ë¡œì§
    
    # ë”ë¯¸ ì ìˆ˜
    if len(response) > 50:
        clarity = 0.80
    else:
        clarity = 0.70
    
    return clarity


def check_completeness(response: str) -> float:
    """
    ì‘ë‹µì˜ ì™„ì „ì„± í‰ê°€
    
    Args:
        response (str): í‰ê°€í•  ì‘ë‹µ
        
    Returns:
        float: ì™„ì „ì„± ì ìˆ˜ (0.0-1.0)
    
    í‰ê°€ ê¸°ì¤€:
        - ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µí–ˆëŠ”ê°€?
        - í•„ìš”í•œ ì •ë³´ê°€ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ê°€?
        - ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨
    
    TODO:
        - ì§ˆë¬¸ ë¶„ì„
        - ë‹µë³€ ìš”ì†Œ ì²´í¬
    """
    # TODO: ì™„ì „ì„± í‰ê°€ ë¡œì§
    
    # ë”ë¯¸ ì ìˆ˜
    if len(response) > 200:
        completeness = 0.85
    else:
        completeness = 0.70
    
    return completeness


def check_safety_guidelines(response: str) -> Dict[str, any]:
    """
    ì‘ë‹µì´ ì•ˆì „ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ í‰ê°€
    
    Args:
        response (str): í‰ê°€í•  ì‘ë‹µ
        
    Returns:
        Dict[str, any]: ì•ˆì „ í‰ê°€ ê²°ê³¼
            {
                'passed': 0.0-1.0,  # ì•ˆì „ì„± ì ìˆ˜
                'has_disclaimer': True/False,  # ë©´ì±… ì¡°í•­ í¬í•¨ ì—¬ë¶€
                'has_emergency_warning': True/False,  # ì‘ê¸‰ ê²½ê³  í¬í•¨ ì—¬ë¶€
                'issues': ['ë¬¸ì œ1', 'ë¬¸ì œ2', ...]  # ë°œê²¬ëœ ë¬¸ì œ
            }
    
    ì•ˆì „ ê¸°ì¤€:
        1. ì˜ë£Œ ë©´ì±… ì¡°í•­ í•„ìˆ˜
           ì˜ˆ: "ì „ë¬¸ ìˆ˜ì˜ì‚¬ ì§„ë£Œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤"
        2. ì‘ê¸‰ ìƒí™© ê²½ê³ 
           ì˜ˆ: "ì‘ê¸‰ ìƒí™©ì´ë©´ ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸"
        3. ê³¼ë„í•œ ì˜ì•½í’ˆ ê¶Œì¥ ê¸ˆì§€
    
    TODO:
        - í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ì‚¬
        - ì˜ë£Œ ì½˜í…ì¸  ê²€ì¦
    """
    # TODO: ì•ˆì „ì„± í‰ê°€ ë¡œì§
    
    print(f"ğŸ›¡ï¸  [check_safety_guidelines] ì•ˆì „ì„± ê²€ì‚¬\n")
    
    # ì•ˆì „ ìš”ì†Œ ê²€ì‚¬
    has_disclaimer = any(kw in response for kw in ['ë©´ì±…', 'ìˆ˜ì˜ì‚¬ ì§„ë£Œ', 'ì „ë¬¸ê°€ ìƒë‹´'])
    has_emergency_warning = any(kw in response for kw in ['ì‘ê¸‰', 'ì¦‰ì‹œ', '119', 'ë³‘ì› ë°©ë¬¸'])
    
    issues = []
    if not has_disclaimer:
        issues.append("ì˜ë£Œ ë©´ì±… ì¡°í•­ ëˆ„ë½")
    if not has_emergency_warning and 'ì¦ìƒ' in response:
        issues.append("ì‘ê¸‰ ê²½ê³  í‘œì‹œ ë¶€ì¡±")
    
    safety_score = 0.9 if has_disclaimer else 0.7
    
    safety_result = {
        'passed': safety_score,
        'has_disclaimer': has_disclaimer,
        'has_emergency_warning': has_emergency_warning,
        'issues': issues
    }
    
    print(f"  - ë©´ì±… ì¡°í•­: {'âœ“' if has_disclaimer else 'âœ—'}")
    print(f"  - ì‘ê¸‰ ê²½ê³ : {'âœ“' if has_emergency_warning else 'âœ—'}")
    print(f"  - ì•ˆì „ë„: {safety_score:.0%}")
    print(f"  - ë¬¸ì œ: {len(issues)}ê°œ\n")
    
    return safety_result


def determine_next_action(
    response: str,
    evaluation: Dict[str, any]
) -> Literal["accept", "rewrite", "escalate"]:
    """
    í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ë‹¤ìŒ ì•¡ì…˜ ê²°ì •
    
    Args:
        response (str): í‰ê°€ëœ ì‘ë‹µ
        evaluation (Dict): í‰ê°€ ê²°ê³¼
        
    Returns:
        Literal["accept", "rewrite", "escalate"]: ë‹¤ìŒ ì•¡ì…˜
            - "accept": ì‘ë‹µ ìŠ¹ì¸ (í‰ê°€ í†µê³¼)
            - "rewrite": ì‘ë‹µ ì¬ì‘ì„± (í‰ê°€ ë¶ˆí†µê³¼, ê°œì„  ê°€ëŠ¥)
            - "escalate": ì—ìŠ¤ì»¬ë ˆì´ì…˜ (í‰ê°€ ì‹¤íŒ¨, ìˆ˜ë™ ê°œì… í•„ìš”)
    
    ì˜ì‚¬ê²°ì • ê¸°ì¤€:
        âœ… accept (í‰ê·  ì ìˆ˜ >= 0.75):
           ì‘ë‹µì´ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        ğŸ”„ rewrite (0.50 <= í‰ê·  ì ìˆ˜ < 0.75):
           ì‘ë‹µì´ ì¼ë¶€ ê°œì„  í•„ìš”, í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„±
        
        âš ï¸  escalate (í‰ê·  ì ìˆ˜ < 0.50):
           ì‘ë‹µì´ ì‹¬ê°í•œ ë¬¸ì œ, ìˆ˜ë™ ê°œì… í•„ìš”
    
    ì˜ˆì‹œ:
        í‰ê°€ ì ìˆ˜ 0.85 â†’ accept
        í‰ê°€ ì ìˆ˜ 0.65 â†’ rewrite
        í‰ê°€ ì ìˆ˜ 0.40 â†’ escalate
    
    TODO:
        - í‰ê°€ ì ìˆ˜ ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì •
        - íŠ¹ìˆ˜ ì¡°ê±´ ì²˜ë¦¬ (ì•ˆì „ì„± ì˜¤ë¥˜ ë“±)
    """
    # TODO: ì˜ì‚¬ê²°ì • ë¡œì§
    
    avg_score = evaluation.get('average_score', 0)
    
    if avg_score >= 0.75:
        action = "accept"
    elif avg_score >= 0.50:
        action = "rewrite"
    else:
        action = "escalate"
    
    print(f"ğŸ¯ [determine_next_action] ì ìˆ˜: {avg_score:.0%} â†’ ì•¡ì…˜: {action}")
    
    return action


def generate_feedback(scores: Dict[str, float], response: str) -> str:
    """
    í‰ê°€ ì ìˆ˜ ê¸°ë°˜ ê°œì„  í”¼ë“œë°± ìƒì„±
    
    Args:
        scores (Dict[str, float]): ì°¨ì›ë³„ í‰ê°€ ì ìˆ˜
        response (str): í‰ê°€ëœ ì‘ë‹µ
        
    Returns:
        str: ê°œì„  í”¼ë“œë°± í…ìŠ¤íŠ¸
    
    í”¼ë“œë°± ì˜ˆì‹œ:
        - "ì •í™•ë„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤" (accuracy < 0.7)
        - "ë„ˆë¬´ ê¸¸ì–´ìš”, ìš”ì•½í•´ì£¼ì„¸ìš”" (length > 500)
        - "ì˜ë£Œ ë©´ì±… ì¡°í•­ì„ ì¶”ê°€í•˜ì„¸ìš”" (safety < 0.7)
    
    TODO:
        - ì ìˆ˜ ê¸°ë°˜ í”¼ë“œë°± ìƒì„±
        - ë§¥ë½í™”ëœ í”¼ë“œë°±
    """
    # TODO: í”¼ë“œë°± ìƒì„± ë¡œì§
    
    feedback_list = []
    
    if scores.get('accuracy', 1.0) < 0.7:
        feedback_list.append("ì •í™•ë„ ê°œì„  í•„ìš”")
    if scores.get('clarity', 1.0) < 0.7:
        feedback_list.append("í‘œí˜„ì´ ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤")
    if scores.get('completeness', 1.0) < 0.7:
        feedback_list.append("ë” ìì„¸í•œ ì„¤ëª… í•„ìš”")
    if scores.get('safety', 1.0) < 0.7:
        feedback_list.append("ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€ í•„ìš”")
    
    if len(response) > 500:
        feedback_list.append("ë‹µë³€ì´ ê¸¸ì–´ìš”")
    
    feedback = " | ".join(feedback_list) if feedback_list else "ì‘ë‹µì´ ìš°ìˆ˜í•©ë‹ˆë‹¤"
    
    return feedback


def collect_evaluation_metrics(
    response: str,
    evaluation: Dict[str, any],
    generation_time: float,
    rewrite_count: int
) -> Dict[str, any]:
    """
    í‰ê°€ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° í†µê³„ ìƒì„±
    
    Args:
        response (str): ìµœì¢… ì‘ë‹µ
        evaluation (Dict): í‰ê°€ ê²°ê³¼
        generation_time (float): ìƒì„± ì‹œê°„ (ì´ˆ)
        rewrite_count (int): ì¬ì‘ì„± íšŸìˆ˜
        
    Returns:
        Dict[str, any]: ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­
            {
                'response_length': 500,
                'generation_time': 2.5,
                'rewrite_count': 1,
                'evaluation_scores': {...},
                'average_score': 0.85,
                'passed_evaluation': True,
                'timestamp': '2025-12-05 10:30:00'
            }
    
    ìˆ˜ì§‘ í•­ëª©:
        - ì‘ë‹µ ê¸¸ì´
        - ìƒì„± ì‹œê°„
        - ì¬ì‘ì„± íšŸìˆ˜
        - í‰ê°€ ì ìˆ˜
        - í†µê³¼ ì—¬ë¶€
        - íƒ€ì„ìŠ¤íƒí”„
    
    ìš©ë„:
        - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        - í†µê³„ ë¶„ì„
        - ë¡œê¹… ë° ê°ì‹œ
    
    TODO:
        - ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¡œì§
        - íƒ€ì„ìŠ¤íƒí”„ ê¸°ë¡
        - ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    """
    # TODO: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥
    
    from datetime import datetime
    
    metrics = {
        'response_length': len(response),
        'generation_time': generation_time,
        'rewrite_count': rewrite_count,
        'evaluation_scores': evaluation.get('scores', {}),
        'average_score': evaluation.get('average_score', 0),
        'passed_evaluation': evaluation.get('pass', False),
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"ğŸ“Š [collect_evaluation_metrics] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"  - ì‘ë‹µ ê¸¸ì´: {metrics['response_length']} ë¬¸ì")
    print(f"  - ìƒì„± ì‹œê°„: {metrics['generation_time']:.2f}ì´ˆ")
    print(f"  - ì¬ì‘ì„±: {metrics['rewrite_count']}íšŒ")
    print(f"  - í‰ê°€ ì ìˆ˜: {metrics['average_score']:.0%}")
    
    return metrics


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)
    """
    
    print("\n" + "="*60)
    print("âš–ï¸  Evaluation Controller Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    # í…ŒìŠ¤íŠ¸ ì‘ë‹µ
    test_response = """ê°•ì•„ì§€ í”¼ë¶€ì—¼ì€ í”¼ë¶€ í‘œë©´ì˜ ì—¼ì¦ì…ë‹ˆë‹¤.
    
ì£¼ìš” ì¦ìƒ:
- ê°€ë ¤ì›€ì¦
- í”¼ë¶€ ë°œì 

ì¹˜ë£Œ ë°©ë²•:
- ì•½ë¬¼ ì¹˜ë£Œ
- í”¼ë¶€ ê´€ë¦¬

âš ï¸  ì´ëŠ” ì¼ë°˜ ì •ë³´ì´ë©° ì „ë¬¸ ìˆ˜ì˜ì‚¬ ì§„ë£Œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."""
    
    print("### í…ŒìŠ¤íŠ¸ 1: í‰ê°€ ###\n")
    evaluation = evaluate_response(test_response)
    print(f"í‰ê°€ ê²°ê³¼: {'âœ… í†µê³¼' if evaluation['pass'] else 'ğŸ”„ ì¬ì‘ì„±'}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 2: ì•ˆì „ì„± ê²€ì‚¬ ###\n")
    safety = check_safety_guidelines(test_response)
    print(f"ì•ˆì „ì„± ì ìˆ˜: {safety['passed']:.0%}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 3: ë‹¤ìŒ ì•¡ì…˜ ê²°ì • ###\n")
    action = determine_next_action(test_response, evaluation)
    print(f"ê²°ì •: {action}\n")
    
    print("\n### í…ŒìŠ¤íŠ¸ 4: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ###\n")
    metrics = collect_evaluation_metrics(test_response, evaluation, 2.5, 1)
    print()
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
