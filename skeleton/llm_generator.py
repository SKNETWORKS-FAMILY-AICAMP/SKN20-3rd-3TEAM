"""
LLM Generator Module
LLMì„ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„± ë° ì¬ì‘ì„±

ì—­í• :
  - í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (ì¿¼ë¦¬ íƒ€ì…ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
  - OpenAI GPT APIë¥¼ í†µí•œ ì‘ë‹µ ìƒì„±
  - í”¼ë“œë°± ê¸°ë°˜ ì‘ë‹µ ì¬ì‘ì„±
  - í† í° ê´€ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ìµœì í™”
"""

from typing import Optional, Dict


def build_system_prompt(query_type: str) -> str:
    """
    ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    
    Args:
        query_type (str): ì¿¼ë¦¬ ë¶„ë¥˜ íƒ€ì…
            - "medical_consultation": ì˜ë£Œ ìƒë‹´
            - "map_search": ì§€ë„ ê²€ìƒ‰
            - "general": ì¼ë°˜ ì§ˆë¬¸
        
    Returns:
        str: êµ¬ì„±ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    
    í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ:
    
    A. Medical Consultation:
       "ë‹¹ì‹ ì€ ìˆ˜ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ì•ˆì „í•œ ì˜ë£Œ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
        ë©´ì±… ì¡°í•­ì„ í¬í•¨í•˜ê³ , ì‘ê¸‰ ìƒí™© íŒë‹¨ ëŠ¥ë ¥ì„ ê°–ì¶”ì„¸ìš”.
        í•­ìƒ 'ìˆ˜ì˜ì‚¬ì˜ ì§„ë£Œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤'ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”."
    
    B. Map Search:
       "ë‹¹ì‹ ì€ ì§€ì—­ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ ìœ„ì¹˜, ê±°ë¦¬, ì˜ì—…ì‹œê°„ì„ ì œê³µí•©ë‹ˆë‹¤.
        ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."
    
    C. General:
       "ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    
    TODO:
        - ì¿¼ë¦¬ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
        - í†¤ ë° ìŠ¤íƒ€ì¼ ì •ì˜
        - íŠ¹ìˆ˜ ì§€ì‹œì‚¬í•­ í¬í•¨
    """
    # TODO: í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¡œì§
    
    prompts = {
        "medical_consultation": """ë‹¹ì‹ ì€ ê²½í—˜ ë§ì€ ìˆ˜ì˜í•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
- ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ë£Œ ì¡°ì–¸ ì œê³µ
- ëª¨ë“  ë‹µë³€ ëì— ë²•ì  ë©´ì±… ì¡°í•­ í¬í•¨: "ì´ëŠ” ì¼ë°˜ ì •ë³´ì´ë©° ì „ë¬¸ ìˆ˜ì˜ì‚¬ ì§„ë£Œë¥¼ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
- ì‘ê¸‰ ìƒí™© ê°ì§€ ëŠ¥ë ¥ ë³´ìœ 
- ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤ ì‚¬ìš©""",
        
        "map_search": """ë‹¹ì‹ ì€ ì§€ì—­ ì •ë³´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
- ì •í™•í•œ ìœ„ì¹˜, ì£¼ì†Œ, ê±°ë¦¬ ì •ë³´ ì œê³µ
- ì˜ì—… ì‹œê°„, ì—°ë½ì²˜ í¬í•¨
- ì‚¬ìš©ì ì¹œí™”ì ì¸ ì§€ë„ ì •ë³´ í˜•ì‹ ì œê³µ
- ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ ì œê³µ""",
        
        "general": """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
- ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ ì œê³µ
- ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€
- í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ ê¶Œìœ 
- ì •ì¤‘í•˜ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€"""
    }
    
    system_prompt = prompts.get(query_type, prompts["general"])
    print(f"âœ“ [build_system_prompt] {query_type} â†’ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ")
    return system_prompt


def generate_response(query: str, context: str) -> str:
    """
    LLMì„ í†µí•´ ìµœì¢… ì‘ë‹µ ìƒì„±
    
    Args:
        query (str): ì‚¬ìš©ì ì¿¼ë¦¬
        context (str): ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (RAG ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê²°ê³¼)
        
    Returns:
        str: LLMì´ ìƒì„±í•œ ìµœì¢… ì‘ë‹µ
    
    ì²˜ë¦¬ ìˆœì„œ:
        1ï¸âƒ£  [í† í° ê´€ë¦¬] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ìµœì í™”
        2ï¸âƒ£  [í”„ë¡¬í”„íŠ¸ êµ¬ì„±] ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        3ï¸âƒ£  [LLM í˜¸ì¶œ] OpenAI GPT API í˜¸ì¶œ
        4ï¸âƒ£  [ì‘ë‹µ ì¶”ì¶œ] API ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        5ï¸âƒ£  [ê²€ì¦] ì‘ë‹µ ê¸¸ì´ ë° í’ˆì§ˆ í™•ì¸
        6ï¸âƒ£  [ë°˜í™˜] ìµœì¢… ì‘ë‹µ ë°˜í™˜
    
    í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:
        ```
        System: {ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸}
        
        Context:
        {ê²€ìƒ‰ ê²°ê³¼}
        
        User: {ì¿¼ë¦¬}
        
        Assistant:
        ```
    
    ì˜ˆì‹œ:
        ì…ë ¥:
        - query: "ê°•ì•„ì§€ í”¼ë¶€ì—¼ ì¦ìƒì´ ë­ì˜ˆìš”?"
        - context: "[RAG ê²€ìƒ‰ ê²°ê³¼] í”¼ë¶€ì—¼ ê´€ë ¨..."
        
        LLM í˜¸ì¶œ:
        - Model: gpt-4o-mini
        - Temperature: 0.7
        - Max tokens: 500
        
        ì¶œë ¥:
        "ê°•ì•„ì§€ í”¼ë¶€ì—¼ì€ í”¼ë¶€ í‘œë©´ì˜ ì—¼ì¦ìœ¼ë¡œ..."
    
    TODO:
        - OpenAI API í˜¸ì¶œ (openai.ChatCompletion.create)
        - í† í° ì œí•œ í™•ì¸
        - ìŠ¤íŠ¸ë¦¬ë° vs ì™„ë£Œ ëª¨ë“œ ì„ íƒ
        - ì—ëŸ¬ ì²˜ë¦¬ (API ì˜¤ë¥˜, íƒ€ì„ì•„ì›ƒ)
    """
    # TODO: LLM API í˜¸ì¶œ
    # client = OpenAI(api_key=OPENAI_API_KEY)
    # response = client.chat.completions.create(
    #     model="gpt-4o-mini",
    #     messages=[
    #         {"role": "system", "content": system_prompt},
    #         {"role": "user", "content": f"Context: {context}\n\nQuestion: {query}"}
    #     ],
    #     temperature=0.7,
    #     max_tokens=500
    # )
    
    print(f"ğŸ¤– [generate_response] LLM ì‘ë‹µ ìƒì„± ì¤‘...")
    print(f"   - ì¿¼ë¦¬: {query[:50]}...")
    print(f"   - ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì\n")
    
    # ë”ë¯¸ ì‘ë‹µ
    response = f"""ê°•ì•„ì§€ í”¼ë¶€ì—¼ì€ í”¼ë¶€ í‘œë©´ì˜ ì—¼ì¦ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì§ˆí™˜ì…ë‹ˆë‹¤.

ì£¼ìš” ì¦ìƒ:
- ê°€ë ¤ì›€ì¦ (ê°€ì¥ í”í•œ ì¦ìƒ)
- í”¼ë¶€ ë°œì ê³¼ ë¶€ì¢…
- íƒˆëª¨ (ì‹¬í•  ê²½ìš°)
- í”¼ë¶€ ì•…ì·¨

ì›ì¸:
- ì•Œë ˆë¥´ê¸° (ìŒì‹, í™˜ê²½)
- ê°ì—¼ (ì„¸ê· , ê³°íŒ¡ì´)
- ê¸°ìƒì¶©

ì¹˜ë£Œ ë°©ë²•:
- ìˆ˜ì˜ì‚¬ ì§„ë£Œ í•„ìˆ˜
- ì•½ë¬¼ ì¹˜ë£Œ (í•­ìƒì œ, í•­ì§„ê· ì œ)
- í”¼ë¶€ ê´€ë¦¬ (ëª©ìš•, ìƒ´í‘¸)
- ì›ì¸ ì œê±° (ì•Œë ˆë¥´ê² íšŒí”¼)

ì˜ˆë°©:
- ì •ê¸°ì ì¸ í”¼ë¶€ ê²€ì§„
- ê· í˜• ì¡íŒ ì‹ë‹¨
- ìœ„ìƒ ê´€ë¦¬

âš ï¸  ì´ëŠ” ì¼ë°˜ ì •ë³´ì´ë©° ì „ë¬¸ ìˆ˜ì˜ì‚¬ ì§„ë£Œë¥¼ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦ìƒì´ ì‹¬í•˜ë©´ ì¦‰ì‹œ ë™ë¬¼ë³‘ì›ì„ ë°©ë¬¸í•˜ì„¸ìš”."""
    
    print(f"âœ“ [generate_response] ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response)} ë¬¸ì)\n")
    return response


def rewrite_response(response: str, feedback: str) -> str:
    """
    í”¼ë“œë°±ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ì¬ì‘ì„±
    
    Args:
        response (str): ì›ë³¸ ì‘ë‹µ
        feedback (str): í‰ê°€ í”¼ë“œë°±
            ì˜ˆ: "ë„ˆë¬´ ê¸¸ì–´ìš”", "ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€ í•„ìš”", "ë” ìì„¸íˆ"
        
    Returns:
        str: ì¬ì‘ì„±ëœ ì‘ë‹µ
    
    ì²˜ë¦¬ ìˆœì„œ:
        1ï¸âƒ£  [í”¼ë“œë°± ë¶„ì„] í”¼ë“œë°± ì˜ë„ ë¶„ì„
        2ï¸âƒ£  [í”„ë¡¬í”„íŠ¸ êµ¬ì„±] ì›ë³¸ ì‘ë‹µê³¼ í”¼ë“œë°±ì„ í¬í•¨í•œ ì¬ì‘ì„± ì§€ì‹œë¬¸
        3ï¸âƒ£  [LLM í˜¸ì¶œ] ìˆ˜ì • ì§€ì‹œë¬¸ê³¼ í•¨ê»˜ LLM í˜¸ì¶œ
        4ï¸âƒ£  [ì‘ë‹µ ì¶”ì¶œ] ì¬ì‘ì„±ëœ ì‘ë‹µ ì¶”ì¶œ
        5ï¸âƒ£  [ë°˜í™˜] ì¬ì‘ì„±ëœ ì‘ë‹µ ë°˜í™˜
    
    ì¬ì‘ì„± ëª…ë ¹ ì˜ˆì‹œ:
    ```
    ì›ë³¸ ì‘ë‹µ:
    {ì›ë³¸ ì‘ë‹µ}
    
    í”¼ë“œë°±: {í”¼ë“œë°±}
    
    ì§€ì‹œ: ìœ„ í”¼ë“œë°±ì„ ê³ ë ¤í•˜ì—¬ ì‘ë‹µì„ ê°œì„ í•˜ì„¸ìš”.
    ```
    
    ì˜ˆì‹œ:
        í”¼ë“œë°±: "ë„ˆë¬´ ê¸¸ì–´ìš”"
        â†’ ì‘ë‹µ ê¸¸ì´ 50% ê°ì¶•, í•µì‹¬ë§Œ ìš”ì•½
        
        í”¼ë“œë°±: "ì˜ë£Œ ë©´ì±… ì¡°í•­ ì¶”ê°€"
        â†’ ë§ˆì§€ë§‰ì— ë©´ì±… ì¡°í•­ ì¶”ê°€
    
    TODO:
        - í”¼ë“œë°± ë¶„ì„ ë° ë³€í™˜
        - ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        - LLM í˜¸ì¶œ
    """
    # TODO: ì¬ì‘ì„± ë¡œì§
    
    print(f"ğŸ”„ [rewrite_response] ì‘ë‹µ ì¬ì‘ì„± ì¤‘...")
    print(f"   í”¼ë“œë°±: {feedback}\n")
    
    # ë”ë¯¸ ì¬ì‘ì„± (ê°„ë‹¨í•œ ë³€í™˜)
    rewritten = f"""{response}

[í”¼ë“œë°± ë°˜ì˜ ìˆ˜ì •]
í”¼ë“œë°± ë‚´ìš©: {feedback}
"""
    
    print(f"âœ“ [rewrite_response] ì¬ì‘ì„± ì™„ë£Œ ({len(rewritten)} ë¬¸ì)\n")
    return rewritten


def estimate_token_count(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •
    
    Args:
        text (str): í† í°ì„ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        
    Returns:
        int: ì¶”ì •ëœ í† í° ìˆ˜
    
    ê·¼ì‚¬ê°’:
        - ì˜ì–´: 1 í† í° â‰ˆ 4 ë¬¸ì
        - í•œêµ­ì–´: 1 í† í° â‰ˆ 1.3 ë¬¸ì (ëª¨ìŒ/ììŒ ë¶„ë¦¬)
    
    ì •í™•í•œ ê³„ì‚°:
        - tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (OpenAI ê³µì‹)
    
    TODO:
        - tiktoken.encoding_for_model("gpt-4o-mini") ì‚¬ìš©
    """
    # TODO: ì •í™•í•œ í† í° ê³„ì‚° (tiktoken)
    # import tiktoken
    # encoding = tiktoken.encoding_for_model("gpt-4o-mini")
    # tokens = encoding.encode(text)
    # return len(tokens)
    
    # ê°„ë‹¨í•œ ê·¼ì‚¬ê°’
    token_count = len(text) // 4  # í‰ê· ì ì¸ ì¶”ì •
    
    print(f"ğŸ“Š [estimate_token_count] {len(text)} ë¬¸ì â†’ ~{token_count} í† í°")
    return token_count


def truncate_context(context: str, max_length: int = 2000) -> str:
    """
    ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê¸°
    
    Args:
        context (str): ì›ë³¸ ì»¨í…ìŠ¤íŠ¸
        max_length (int): ìµœëŒ€ ê¸¸ì´ (ë¬¸ì ìˆ˜)
        
    Returns:
        str: ì˜ë¦° ì»¨í…ìŠ¤íŠ¸
    
    ì „ëµ:
        1. ê¸¸ì´ í™•ì¸
        2. í•„ìš”ì‹œ ëì—ì„œ ì˜ë¼ë‚´ê¸°
        3. ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ í¬í•¨
    
    TODO:
        - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
        - ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (ë” ë‚˜ìŒ)
    """
    # TODO: ì»¨í…ìŠ¤íŠ¸ íŠ¸ëŸ°ì¼€ì´ì…˜ ë¡œì§
    
    if len(context) <= max_length:
        print(f"âœ“ [truncate_context] ì´ë¯¸ ìµœëŒ€ ê¸¸ì´ ì´ë‚´ ({len(context)}/{max_length})")
        return context
    
    # ëì—ì„œë¶€í„° ìë¥´ê¸° (ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€)
    truncated = context[:max_length]
    last_period = truncated.rfind('.')
    if last_period > max_length * 0.8:  # 80% ì´ìƒì˜ ë‚´ìš© ë³´ì¡´
        truncated = truncated[:last_period + 1]
    
    print(f"âœ“ [truncate_context] {len(context)} â†’ {len(truncated)} ë¬¸ì (ì œí•œ: {max_length})")
    return truncated


def calculate_token_cost(
    input_tokens: int,
    output_tokens: int,
    model: str = "gpt-4o-mini"
) -> float:
    """
    API í˜¸ì¶œì˜ ë¹„ìš© ê³„ì‚°
    
    Args:
        input_tokens (int): ì…ë ¥ í† í° ìˆ˜
        output_tokens (int): ì¶œë ¥ í† í° ìˆ˜
        model (str): ëª¨ë¸ ì´ë¦„
        
    Returns:
        float: ì˜ˆìƒ ë¹„ìš© (USD)
    
    ê°€ê²©í‘œ (ì˜ˆì‹œ):
        gpt-4o-mini:
        - ì…ë ¥: $0.15 / 1M í† í°
        - ì¶œë ¥: $0.60 / 1M í† í°
    
    TODO:
        - ìµœì‹  ê°€ê²©í‘œ ìœ ì§€
        - ëª¨ë¸ë³„ ê°€ê²© ì •ë³´
    """
    # ë”ë¯¸ ê°€ê²© ê³„ì‚°
    input_cost = (input_tokens / 1_000_000) * 0.00015
    output_cost = (output_tokens / 1_000_000) * 0.0006
    total_cost = input_cost + output_cost
    
    print(f"ğŸ’° [calculate_token_cost] ì˜ˆìƒ ë¹„ìš©: ${total_cost:.6f}")
    return total_cost


# ==================== ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸ ====================
if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ì¼ˆë ˆí†¤ ë°ëª¨)
    """
    
    print("\n" + "="*60)
    print("ğŸ¤– LLM Generator Module - í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    # í…ŒìŠ¤íŠ¸ 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    print("### í…ŒìŠ¤íŠ¸ 1: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ###\n")
    medical_prompt = build_system_prompt("medical_consultation")
    print(f"{medical_prompt[:100]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 2: ì‘ë‹µ ìƒì„±
    print("### í…ŒìŠ¤íŠ¸ 2: LLM ì‘ë‹µ ìƒì„± ###\n")
    test_query = "ê°•ì•„ì§€ í”¼ë¶€ì—¼ì´ ë­ì˜ˆìš”?"
    test_context = "[ê²€ìƒ‰ ê²°ê³¼] í”¼ë¶€ì—¼ ê´€ë ¨ ì˜ë£Œ ì •ë³´..."
    response = generate_response(test_query, test_context)
    print(f"{response[:200]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 3: ì‘ë‹µ ì¬ì‘ì„±
    print("### í…ŒìŠ¤íŠ¸ 3: ì‘ë‹µ ì¬ì‘ì„± ###\n")
    feedback = "ë„ˆë¬´ ê¸¸ì–´ìš”, ìš”ì•½í•´ì£¼ì„¸ìš”"
    rewritten = rewrite_response(response, feedback)
    print(f"{rewritten[:200]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 4: í† í° ì¶”ì •
    print("### í…ŒìŠ¤íŠ¸ 4: í† í° ê³„ì‚° ###\n")
    tokens = estimate_token_count(response)
    print(f"ì˜ˆìƒ í† í°: {tokens}\n")
    
    # í…ŒìŠ¤íŠ¸ 5: ë¹„ìš© ê³„ì‚°
    print("### í…ŒìŠ¤íŠ¸ 5: ë¹„ìš© ê³„ì‚° ###\n")
    calculate_token_cost(100, 300)
    print()
    
    print("="*60)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*60)
