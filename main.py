"""
ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ë°˜ë ¤ë™ë¬¼ ì „ë¬¸ QA ë° ë³‘ì› ì•ˆë‚´ ì–´ì‹œìŠ¤í„´íŠ¸
"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# src ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent))
from src.ingestion import ingest_data
from src.chunking import chunk_documents_with_token_range
from src.embeddings import get_embedding_model, create_vectorstore, load_vectorstore
from src.advanced_rag_pipeline import AdvancedRAGPipeline


def setup_advanced_rag_system(
    data_dir: str = "data/raw/disease",
    persist_directory: str = "./chroma_db",
    collection_name: str = "rag_collection",
    embedding_model_type: str = "openai",
    rebuild_vectorstore: bool = False
):
    """
    ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì„¤ì •
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        persist_directory: ë²¡í„° DB ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        embedding_model_type: ì„ë² ë”© ëª¨ë¸ íƒ€ì… ("openai" ë˜ëŠ” "huggingface")
        rebuild_vectorstore: ë²¡í„°ìŠ¤í† ì–´ ì¬êµ¬ì¶• ì—¬ë¶€
        
    Returns:
        AdvancedRAGPipeline ê°ì²´
    """
    print("=" * 80)
    print("ğŸš€ ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
    print("=" * 80)
    
    # 1. ì„ë² ë”© ëª¨ë¸ ìƒì„±
    print("\n[1/4] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = get_embedding_model(embedding_model_type)
    print(f"âœ“ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model_type}")
    
    # 2. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
    print("\n[2/4] ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬ ì¤‘...")
    if rebuild_vectorstore or not os.path.exists(persist_directory):
        # ë°ì´í„° ingestion
        print("\n[2-1] ë°ì´í„° ingestion ì¤‘...")
        documents = ingest_data(data_dir)
        
        # Chunking
        print("\n[2-2] ë¬¸ì„œ chunking ì¤‘...")
        chunked_docs = chunk_documents_with_token_range(
            documents,
            min_tokens=300,
            max_tokens=500,
            overlap_ratio=0.25
        )
        print(f"âœ“ {len(chunked_docs)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        print("\n[2-3] ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        vectorstore = create_vectorstore(
            chunked_docs,
            embedding_model,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
        print("âœ“ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    else:
        # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        vectorstore = load_vectorstore(
            embedding_model,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
        print("âœ“ ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ìƒì„±
    print("\n[3/4] ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
    pipeline = AdvancedRAGPipeline(
        vectorstore=vectorstore,
        hospital_json_path="data/raw/hospital/ì„œìš¸ì‹œ_ë™ë¬¼ë³‘ì›_ì¸í—ˆê°€_ì •ë³´.json",
        llm_model="gpt-4o-mini",
        score_threshold=0.6
    )
    print("âœ“ íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ (ë¶„ë¥˜ + ì˜ë£Œ + ë³‘ì› + ì¼ë°˜ ì²˜ë¦¬)")
    
    print("\n[4/4] ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 80)
    
    return pipeline


def run_example_queries():
    """
    ì˜ˆì‹œ ì§ˆë¬¸ ì‹¤í–‰
    """
    example_queries = [
        # ì˜ë£Œ ì§ˆë¬¸ (Type A)
        "ê°œì˜ í”¼ë¶€ì—¼ ì¦ìƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ê°•ì•„ì§€ê°€ êµ¬í† ë¥¼ í•˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê³ ì–‘ì´ì˜ ì‹ ë¶€ì „ ì¹˜ë£Œë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        
        # ë³‘ì› ì§ˆë¬¸ (Type B)
        "ê°•ë‚¨êµ¬ì˜ ë™ë¬¼ë³‘ì›ì„ ì°¾ì•„ì£¼ì„¸ìš”.",
        "ì„œìš¸ì—ì„œ 24ì‹œê°„ ì‘ê¸‰ì§„ë£Œë¥¼ í•˜ëŠ” ë³‘ì›ì´ ìˆë‚˜ìš”?",
        
        # ì¼ë°˜ ì§ˆë¬¸ (Type C)
        "ë°˜ë ¤ë™ë¬¼ì„ ì²˜ìŒ í‚¤ìš°ëŠ”ë° ì–´ë–¤ ì¤€ë¹„ê°€ í•„ìš”í•œê°€ìš”?",
    ]
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ ì˜ˆì‹œ ì§ˆë¬¸ ì‹¤í–‰")
    print("=" * 80)
    
    pipeline = setup_advanced_rag_system()
    
    for i, query in enumerate(example_queries, 1):
        print(f"\n{'=' * 80}")
        print(f"[ì˜ˆì‹œ {i}] {query}")
        print("=" * 80)
        
        result = pipeline.process_question(query)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“ ë‹µë³€:")
        print(result['formatted_answer'])
        
        # ë¶„ë¥˜ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ë¥˜ ì •ë³´:")
        print(f"  ìœ í˜•: {result['classification_type']}")
        print(f"  ì‹ ë¢°ë„: {result['classification_confidence']:.2%}")
        print(f"  ì‚¬ìœ : {result['classification_reason']}")
        
        print("\n" + "-" * 80)
        input("ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")


def run_batch_queries_from_file():
    """
    íŒŒì¼ì—ì„œ ì§ˆë¬¸ì„ ì½ì–´ ë°°ì¹˜ ì²˜ë¦¬
    """
    query_file = "queries.txt"
    
    if not os.path.exists(query_file):
        print(f"ì¿¼ë¦¬ íŒŒì¼ '{query_file}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(query_file, 'r', encoding='utf-8') as f:
        queries = [line.strip() for line in f if line.strip()]
    
    print(f"\n{len(queries)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    pipeline = setup_advanced_rag_system()
    results = pipeline.batch_process_questions(queries)
    
    # ê²°ê³¼ ì €ì¥
    pipeline.save_results(results, "batch_results.json")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
        input("\nê³„ì† ì§„í–‰í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    
    # RAG ì‹œìŠ¤í…œ ì„¤ì •
    pipeline = setup_advanced_rag_system(
        data_dir="data/Validation/01.ì›ì²œë°ì´í„°",
        persist_directory="./chroma_db",
        collection_name="rag_collection",
        embedding_model_type="openai",
        rebuild_vectorstore=False
    )
    
    # ë©”ë‰´ í‘œì‹œ
    print("\n" + "=" * 80)
    print("ğŸ¾ ë°˜ë ¤ë™ë¬¼ ì „ë¬¸ QA ë° ë³‘ì› ì•ˆë‚´ ì–´ì‹œìŠ¤í„´íŠ¸")
    print("=" * 80)
    print("\në©”ë‰´:")
    print("  1. ì˜ˆì‹œ ì§ˆë¬¸ ì‹¤í–‰")
    print("  2. ëŒ€í™”í˜• ëª¨ë“œ")
    print("  3. ë°°ì¹˜ ì²˜ë¦¬ (queries.txtì—ì„œ ì½ê¸°)")
    print("  4. ì¢…ë£Œ")
    print("=" * 80)
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        run_example_queries()
    elif choice == "2":
        pipeline.interactive_mode()
    elif choice == "3":
        run_batch_queries_from_file()
    elif choice == "4":
        print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        print("ìœ íš¨í•œ ì„ íƒì´ ì•„ë‹™ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

